{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "####################### Machine Learning #####################\n",
        "\n",
        "############################## CountVectorizer\n",
        "\n",
        "# Bibliotheken installieren (für die Nutzung von pandas und scikit-learn)\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Importieren der notwendigen Bibliotheken\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Textdaten definieren (Beispieldaten)\n",
        "text = ['This is my bag',  # Satz 1\n",
        "        'I like this movie',  # Satz 2\n",
        "        'She wants to buy my bag']  # Satz 3\n",
        "\n",
        "# Initialisierung des CountVectorizers zur Umwandlung von Textdaten in numerische Merkmale\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Erstellen einer Sparse-Matrix aus den Textdaten\n",
        "matrix = cv.fit_transform(text)\n",
        "\n",
        "# Die Matrix repräsentiert die Häufigkeiten der Wörter im Text\n",
        "print(matrix)  # Ausgabe: <3x10 sparse matrix mit 13 gespeicherten Elementen>\n",
        "matrix\n",
        "#   (0, 7)\t1\n",
        "#   (0, 2)\t1\n",
        "#   (0, 5)\t1\n",
        "#   (0, 0)\t1\n",
        "#   (1, 7)\t1\n",
        "#   (1, 3)\t1\n",
        "#   (1, 4)\t1\n",
        "#   (2, 5)\t1\n",
        "#   (2, 0)\t1\n",
        "#   (2, 6)\t1\n",
        "#   (2, 9)\t1\n",
        "#   (2, 8)\t1\n",
        "#   (2, 1)\t1\n",
        "# <3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
        "# \twith 13 stored elements in Compressed Sparse Row format>\n",
        "\n",
        "# Umwandlung der Sparse-Matrix in ein Array zur besseren Ansicht\n",
        "array = matrix.toarray()\n",
        "print(array)  # Numerische Darstellung der Textdaten\n",
        "array\n",
        "# array([[1, 0, 1, 0, 0, 1, 0, 1, 0, 0],\n",
        "#        [0, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
        "#        [1, 1, 0, 0, 0, 1, 1, 0, 1, 1]])\n",
        "\n",
        "# Erstellen eines DataFrames mit den Spaltennamen (Wörter aus dem Text)\n",
        "df = pd.DataFrame(data=array, columns=cv.get_feature_names_out())\n",
        "print(df)  # Ausgabe der Wort-Häufigkeiten je Satz\n",
        "#    bag  buy  is  like  movie  my  she  this  to  wants\n",
        "# 0    1    0   1     0      0   1    0     1   0      0\n",
        "# 1    0    0   0     1      1   0    0     1   0      0\n",
        "# 2    1    1   0     0      0   1    1     0   1      1\n",
        "\n",
        "\n",
        "############################################### naive Bayes\n",
        "\n",
        "\n",
        "\n",
        "# Weitere Bibliotheken importieren für maschinelles Lernen\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Zugriff auf Google Drive (benötigt Colab)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# CSV-Daten laden\n",
        "# Diese Datei enthält Textdaten und zugehörige Klassen\n",
        "\n",
        "# Daten laden (Anpassen des Pfads, falls erforderlich)\n",
        "df = pd.read_csv('/content/drive/My Drive/bbc_data.csv')\n",
        "\n",
        "\n",
        "# Anzeige der ersten Zeilen des DataFrames\n",
        "print(df.head())\n",
        "df.head()\n",
        "# Mounted at /content/drive\n",
        "# text\tclass\n",
        "# 0\tMusicians to tackle US red tape Musicians gro...\tentertainment\n",
        "# 1\tU2s desire to be number one U2, who have won ...\tentertainment\n",
        "# 2\tRocker Doherty in on-stage fight Rock singer ...\tentertainment\n",
        "# 3\tSnicket tops US box office chart The film ada...\tentertainment\n",
        "# 4\tOceans Twelve raids box office Oceans Twelve,...\tentertainment\n",
        "# ...\n",
        "\n",
        "# Ausgabe der Dimensionen des DataFrames\n",
        "print(df.shape)  # (2225, 2)\n",
        "df.shape\n",
        "\n",
        "# Definieren der Eingabedaten (X) und Zielklassen (y)\n",
        "# X enthält die Texte (Merkmale), y enthält die zugehörigen Klassen (Labels)\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Verteilung der Klassen analysieren\n",
        "# Diese Zeile zeigt, wie viele Datenpunkte jeder Klasse (Kategorie) zugeordnet sind.\n",
        "df['class'].value_counts()\n",
        "# print(df['class'].value_counts())\n",
        "# class\n",
        "# sport            511\n",
        "# business         510\n",
        "# politics         417\n",
        "# tech             401\n",
        "# entertainment    386\n",
        "# Name: count, dtype: int64\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testdatensätze\n",
        "# 70% der Daten werden für das Training verwendet, 30% für das Testen.\n",
        "# random_state sorgt dafür, dass die Aufteilung reproduzierbar ist.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Umwandeln der Textdaten in numerische Merkmale mit CountVectorizer\n",
        "# CountVectorizer wandelt Texte in numerische Matrizen um, basierend auf der Häufigkeit der Wörter.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# fit_transform wird auf die Trainingsdaten angewendet, um die Wort-Häufigkeiten zu lernen\n",
        "# und gleichzeitig die Trainingsdaten in eine numerische Darstellung zu transformieren.\n",
        "X_train_vectorized = cv.fit_transform(X_train)\n",
        "\n",
        "# transform wird auf die Testdaten angewendet, um sie basierend auf der Struktur der Trainingsdaten zu transformieren,\n",
        "# ohne neue Wort-Häufigkeiten zu lernen (um Datenleckage zu vermeiden).\n",
        "X_test_vectorized = cv.transform(X_test)\n",
        "\n",
        "# Initialisierung und Training des Naive Bayes Klassifikators\n",
        "# MultinomialNB ist besonders geeignet für diskrete Daten wie Wortzählungen.\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Training des Modells mit den vektorisierten Trainingsdaten und den zugehörigen Klassen (y_train).\n",
        "classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Vorhersagen auf den Testdaten\n",
        "# Das trainierte Modell wird verwendet, um die Klassen der Testdaten vorherzusagen.\n",
        "y_pred = classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Berechnung und Ausgabe der Genauigkeit des Modells\n",
        "# accuracy_score vergleicht die vorhergesagten Klassen (y_pred) mit den tatsächlichen Klassen (y_test)\n",
        "# und berechnet den Anteil korrekt klassifizierter Datenpunkte.\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # Accuracy: 0.97904191616766\n",
        "\n",
        "\n",
        "\n",
        "############################################### confusion Matrix\n",
        "\n",
        "\n",
        "# Die Confusion Matrix (oder auch \"Fehlermatrix\") ist ein zentrales Werkzeug in der\n",
        "# Bewertung von Machine-Learning-Modellen, insbesondere für Klassifikationsaufgaben.\n",
        "# Sie gibt einen Überblick darüber, wie gut das Modell die Klassen vorhergesagt hat,\n",
        "# indem es die tatsächlichen und vorhergesagten Werte in einer Matrix gegenüberstellt.\n",
        "# Evaluierung des Modells mit einer Konfusionsmatrix\n",
        "# Die Konfusionsmatrix ist ein hilfreiches Werkzeug, um die Leistung eines Klassifikators zu evaluieren.\n",
        "# Sie zeigt, wie oft die vorhergesagten Klassen mit den tatsächlichen Klassen übereinstimmen.\n",
        "# Die Zeilen repräsentieren die tatsächlichen Klassen, die Spalten die vorhergesagten Klassen.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Berechnung der Konfusionsmatrix auf Basis der Testdaten (y_test) und der Vorhersagen (y_pred).\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Ausgabe der Konfusionsmatrix als numerisches Array.\n",
        "# Jede Zelle (i, j) gibt die Anzahl der Datenpunkte an, die zur Klasse i gehörten, aber als Klasse j vorhergesagt wurden.\n",
        "print(cm)  # Ausgabe der Konfusionsmatrix\n",
        "cm\n",
        "# array([[157,   0,   2,   0,   4],\n",
        "#        [  1, 117,   1,   0,   1],\n",
        "#        [  1,   0, 111,   0,   0],\n",
        "#        [  1,   0,   0, 147,   0],\n",
        "#        [  1,   1,   1,   0, 122]])\n",
        "\n",
        "# Visualisierung der Konfusionsmatrix mit Plotly\n",
        "# Plotly Express wird verwendet, um die Matrix als Heatmap darzustellen.\n",
        "import plotly.express as px\n",
        "\n",
        "# Erstellung einer Heatmap basierend auf der Konfusionsmatrix\n",
        "fig = px.imshow(\n",
        "    cm,  # Die numerischen Werte der Konfusionsmatrix\n",
        "    labels=dict(\n",
        "        x=\"Vorhergesagte Werte\",  # Beschriftung der x-Achse (Vorhersagen)\n",
        "        y=\"Tatsächliche Werte\",  # Beschriftung der y-Achse (Tatsächliche Klassen)\n",
        "        color=\"Häufigkeit\"  # Farbskala repräsentiert die Anzahl der Datenpunkte\n",
        "    ),\n",
        "    x=['business', 'entertainment', 'politics', 'sport', 'tech'],  # Klassen für die x-Achse\n",
        "    y=['business', 'entertainment', 'politics', 'sport', 'tech']   # Klassen für die y-Achse\n",
        ")\n",
        "\n",
        "# Anpassung der x-Achse: Labels werden unten angezeigt\n",
        "fig.update_xaxes(side=\"bottom\")\n",
        "\n",
        "# Hinzufügen der numerischen Werte direkt in die Heatmap\n",
        "# texttemplate wird verwendet, um die Werte der Matrix in jeder Zelle anzuzeigen.\n",
        "fig.update_traces(text=cm, texttemplate=\"%{text}\")\n",
        "\n",
        "# Anzeige der Heatmap\n",
        "fig.show()\n",
        "\n",
        "# Wichtige Metriken aus der Confusion Matrix\n",
        "# Mit diesen Werten können verschiedene Metriken berechnet werden,\n",
        "# die die Leistung des Modells bewerten:\n",
        "# Accuracy (Genauigkeit):\n",
        "# Der Anteil der korrekt vorhergesagten Fälle (sowohl positive als auch negative)\n",
        "# an allen Fällen\n",
        "# Precision (Präzision):\n",
        "# Der Anteil der korrekt als positiv klassifizierten Fälle an allen als positiv\n",
        "# vorhergesagten Fällen (Wie viele der vorhergesagten Positiven sind tatsächlich positiv?)\n",
        "# Recall (Sensitivität, Trefferquote):\n",
        "# Der Anteil der korrekt als positiv klassifizierten Fälle an allen tatsächlich\n",
        "# positiven Fällen (Wie viele der tatsächlichen Positiven hat das Modell gefunden?)\n",
        "# F1-Score:\n",
        "# Das harmonische Mittel von Precision und Recall, das ein Gleichgewicht zwischen\n",
        "# beiden schafft\n",
        "# Specificity (Spezifität):\n",
        "# Der Anteil der korrekt als negativ klassifizierten Fälle an allen tatsächlich\n",
        "# negativen Fällen (Wie gut erkennt das Modell die negativen Fälle?)\n",
        "\n",
        "# Interpretation der Werte:\n",
        "# False Positives (FP):\n",
        "# Ein hoher FP-Wert bedeutet, dass das Modell oft \"falschen Alarm\" gibt.\n",
        "# Das kann in sicherheitskritischen Bereichen (z. B. Krebsdiagnose) problematisch sein.\n",
        "# False Negatives (FN):\n",
        "# Ein hoher FN-Wert bedeutet, dass das Modell oft wichtige positive Fälle übersieht.\n",
        "# Dies ist z. B. bei medizinischen Anwendungen kritisch,\n",
        "# da Erkrankungen unerkannt bleiben könnten.\n",
        "# True Positives (TP) und True Negatives (TN):\n",
        "# Diese Werte zeigen, wie oft das Modell korrekt arbeitet.\n",
        "# Höhere Werte sind natürlich wünschenswert.\n",
        "\n",
        "# Ausgabe des Klassifikationsberichts\n",
        "# Der Klassifikationsbericht enthält detaillierte Metriken für jede Klasse:\n",
        "# - Precision: Wie viele der vorhergesagten Instanzen einer Klasse korrekt waren.\n",
        "# - Recall: Wie viele der tatsächlichen Instanzen einer Klasse korrekt erkannt wurden.\n",
        "# - F1-Score: Harmonisches Mittel aus Precision und Recall.\n",
        "# - Support: Anzahl der tatsächlichen Instanzen pro Klasse.\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Classification Report:\n",
        "#                 precision    recall  f1-score   support\n",
        "#\n",
        "#      business       0.98      0.96      0.97       163\n",
        "# entertainment       0.99      0.97      0.98       120\n",
        "#      politics       0.97      0.99      0.98       112\n",
        "#         sport       1.00      0.99      1.00       148\n",
        "#          tech       0.96      0.98      0.97       125\n",
        "#\n",
        "#      accuracy                           0.98       668\n",
        "#     macro avg       0.98      0.98      0.98       668\n",
        "#  weighted avg       0.98      0.98      0.98       668\n",
        "\n",
        "\n",
        "# 1. Precision (Präzision)\n",
        "# Definition: Precision misst, wie viele der als positiv vorhergesagten Fälle\n",
        "# tatsächlich positiv sind. Hohe Precision: Das Modell hat wenige False Positives.\n",
        "# Relevant, wenn False Positives kritisch sind (z. B. bei einem Spam-Filter,\n",
        " #                                      der nur echte Spam-Mails markieren soll).\n",
        "\n",
        "# 2. Recall (Trefferquote/Sensitivität)\n",
        "# Definition: Recall misst, wie viele der tatsächlich positiven Fälle vom Modell\n",
        "# korrekt erkannt wurden.\n",
        "# Interpretation:\n",
        "# Hoher Recall: Das Modell übersieht wenige tatsächliche positive Fälle.\n",
        "# Relevant, wenn False Negatives kritisch sind (z. B. bei einer Krebsdiagnose,\n",
        " #                                  bei der jede Erkrankung erkannt werden muss).\n",
        "\n",
        "# 3. F1-Score\n",
        "# Definition: Der F1-Score ist das harmonische Mittel von Precision und Recall.\n",
        "# Er balanciert die beiden Metriken aus.\n",
        "# Interpretation:\n",
        "# Der F1-Score ist besonders nützlich, wenn Precision und Recall beide wichtig sind.\n",
        "# Er ist weniger geeignet, wenn ein klares Ungleichgewicht in der Wichtigkeit\n",
        "# zwischen Precision und Recall besteht.\n",
        "\n",
        "# 4. Support\n",
        "# Definition: Support ist die Anzahl der tatsächlichen Beispiele in einer Klasse.\n",
        "# Interpretation:\n",
        "# Der Support gibt keine Information über die Modellqualität, sondern beschreibt die\n",
        "# Datenverteilung. Eine stark unausgewogene Verteilung kann das Modell beeinflussen.\n",
        "# Globale Metriken im Classification Report\n",
        "# Zusätzlich zu den Metriken für jede Klasse werden oft globale Metriken berechnet:\n",
        "\n",
        "# 1. Macro Average\n",
        "# Definition: Der ungewichtete Durchschnitt der Metriken über alle Klassen.\n",
        "# Interpretation:\n",
        "# Berücksichtigt keine Klassenhäufigkeiten.\n",
        "# Gut geeignet, wenn alle Klassen gleich wichtig sind.\n",
        "\n",
        "# 2. Weighted Average\n",
        "# Definition: Der gewichtete Durchschnitt der Metriken, wobei der Support\n",
        "#  (Anzahl der Beispiele) der Klassen als Gewicht verwendet wird.\n",
        "# Interpretation:\n",
        "# Berücksichtigt Klassenhäufigkeiten.\n",
        "# Nützlich bei unausgewogenen Datensätzen\n",
        "\n",
        "# Praktische Bedeutung der Metriken\n",
        "# Precision hoch, Recall niedrig: Das Modell ist vorsichtig und klassifiziert nur die sichersten positiven Fälle, übersieht aber einige. (Nützlich z. B. für Spam-Filter.)\n",
        "# Precision niedrig, Recall hoch: Das Modell versucht, alle positiven Fälle zu finden, generiert aber viele Fehlalarme. (Nützlich z. B. in der Medizin, um möglichst keine Erkrankung zu übersehen.)\n",
        "# F1-Score: Gibt eine balancierte Bewertung von Precision und Recall. Nützlich, wenn weder Precision noch Recall allein ausreichend sind.\n",
        "# Precision für Klasse 0 (0.75): 75% der vorhergesagten positiven Fälle in Klasse 0 sind korrekt.\n",
        "# Recall für Klasse 1 (0.80): 80% der tatsächlichen positiven Fälle in Klasse 1 wurden korrekt erkannt.\n",
        "# Accuracy (70%): Der Anteil der insgesamt korrekt klassifizierten Fälle\n",
        "\n",
        "\n",
        "########################################### TF-IDF\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Define the corpus\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# Beispiel einer TF-IDF Transformation\n",
        "# TF-IDF steht für \"Term Frequency-Inverse Document Frequency\" und misst die Relevanz eines Wortes in einem Dokument relativ zu allen anderen Dokumenten im Korpus.\n",
        "# Es ist eine gängige Methode zur numerischen Darstellung von Textdaten.\n",
        "\n",
        "# Initialisierung des TF-IDF Vektorisierers\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "# Transformation des Textkorpus (\"corpus\") in eine TF-IDF Repräsentation\n",
        "# Der fit_transform() Prozess:\n",
        "# - fit(): Lernt die Gewichtung der Begriffe basierend auf ihrem Vorkommen im Korpus.\n",
        "# - transform(): Wandelt den Text in eine numerische Darstellung basierend auf den gelernten Gewichtungen um.\n",
        "tf_idf = vec.fit_transform(corpus)\n",
        "\n",
        "# Ausgabe der TF-IDF Matrix als DataFrame\n",
        "# - toarray(): Wandelt die sparse matrix in ein denses Array um, um die Werte sichtbar zu machen.\n",
        "# - get_feature_names_out(): Gibt die Namen der Merkmale (Wörter) zurück.\n",
        "import pandas as pd\n",
        "print(pd.DataFrame(tf_idf.toarray(), columns=vec.get_feature_names_out()))\n",
        "\n",
        "#         and  document     first        is       one    second       the  \\\n",
        "# 0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085\n",
        "# 1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089\n",
        "# 2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104\n",
        "# 3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085\n",
        "#\n",
        "#       third      this\n",
        "# 0  0.000000  0.384085\n",
        "# 1  0.000000  0.281089\n",
        "# 2  0.511849  0.267104\n",
        "# 3  0.000000  0.384085\n",
        "\n",
        "#################################### naive Bayes with TF-IDF vectors\n",
        "\n",
        "# Ein multifaktorielles Naive-Bayes-Modell (auch bekannt als Naive Bayes Classifier)\n",
        "# ist ein einfacher, aber leistungsstarker Algorithmus, der oft für Klassifikationsprobleme\n",
        "# verwendet wird. Er basiert auf der Anwendung des Bayes-Theorems unter der Annahme,\n",
        "# dass die Merkmale voneinander unabhängig sind (daher der Begriff \"naiv\").\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Importieren der notwendigen Module aus scikit-learn\n",
        "from sklearn.model_selection import train_test_split  # Zum Aufteilen der Daten in Trainings- und Testdatensätze\n",
        "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Klassifikator für Multinomialdaten\n",
        "from sklearn.metrics import accuracy_score, classification_report  # Evaluierungsmethoden wie Genauigkeit und Bericht\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF Vektorisierung für Textdaten\n",
        "\n",
        "# Zugriff auf Google Drive herstellen (benötigt Google Colab)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')  # Mountet Google Drive, um Zugriff auf Dateien zu ermöglichen\n",
        "\n",
        "# Laden der BBC-Daten aus einer CSV-Datei\n",
        "# Die CSV-Datei enthält zwei Spalten: 'text' (Artikeltexte) und 'class' (Kategorien der Artikel)\n",
        "df = pd.read_csv('/content/drive/My Drive/bbc_data.csv')\n",
        "\n",
        "# Definition der Eingabedaten (X) und Zielklassen (y)\n",
        "# X: Artikeltexte; y: zugehörige Kategorien.\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testdatensätze\n",
        "# Trainingsdaten: 70%, Testdaten: 30% der gesamten Daten\n",
        "# random_state sorgt für Reproduzierbarkeit der Aufteilung\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# TF-IDF Vektorisierung der Textdaten\n",
        "# TF-IDF (Term Frequency-Inverse Document Frequency) misst die Relevanz eines Wortes innerhalb eines Dokuments relativ zu anderen Dokumenten\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "# Anpassen und Transformieren der Trainingsdaten\n",
        "# fit_transform: Lernt die Gewichtungen der Wörter und wandelt die Texte in numerische Merkmale um\n",
        "X_train_vectorized = vec.fit_transform(X_train)\n",
        "\n",
        "# Transformieren der Testdaten basierend auf den Trainingsdaten\n",
        "# transform: Wandelt die Texte in numerische Merkmale um, ohne neue Gewichtungen zu lernen\n",
        "X_test_vectorized = vec.transform(X_test)\n",
        "\n",
        "# Initialisierung und Training des Naive Bayes Modells\n",
        "# MultinomialNB: Probabilistisches Modell für diskrete Daten wie Wortzählungen\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_vectorized, y_train)  # Anpassen des Modells an die Trainingsdaten\n",
        "\n",
        "# Vorhersagen auf den Testdaten\n",
        "y_pred = classifier.predict(X_test_vectorized)  # Vorhersagen der Kategorien für die Testdaten\n",
        "\n",
        "# Berechnung der Genauigkeit des Modells\n",
        "# accuracy_score: Misst den Anteil korrekt klassifizierter Datenpunkte\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # Accuracy: 0.9431137724550899\n",
        "\n",
        "# Berechnung der Konfusionsmatrix\n",
        "# Die Konfusionsmatrix zeigt die Verteilung der vorhergesagten Klassen im Vergleich zu den tatsächlichen Klassen\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualisierung der Konfusionsmatrix\n",
        "# Plotly Express wird verwendet, um die Konfusionsmatrix als Heatmap darzustellen\n",
        "import plotly.express as px\n",
        "fig = px.imshow(\n",
        "    cm,  # Numerische Werte der Konfusionsmatrix\n",
        "    labels=dict(\n",
        "        x=\"Vorhergesagte Werte\",  # Beschriftung der x-Achse (Vorhersagen)\n",
        "        y=\"Tatsächliche Werte\",  # Beschriftung der y-Achse (Tatsächliche Klassen)\n",
        "        color=\"Häufigkeit\"  # Farbskala repräsentiert die Häufigkeit der Werte\n",
        "    ),\n",
        "    x=['business', 'entertainment', 'politics', 'sport', 'tech'],  # Klassen für die x-Achse\n",
        "    y=['business', 'entertainment', 'politics', 'sport', 'tech']   # Klassen für die y-Achse\n",
        ")\n",
        "\n",
        "# Anpassung der x-Achse: Labels werden unten angezeigt\n",
        "fig.update_xaxes(side=\"bottom\")\n",
        "\n",
        "# Hinzufügen der numerischen Werte direkt in die Heatmap\n",
        "fig.update_traces(text=cm, texttemplate=\"%{text}\")\n",
        "\n",
        "# Anzeige der Heatmap\n",
        "fig.show()\n",
        "\n",
        "# Ausgabe des Klassifikationsberichts\n",
        "# Der Bericht enthält:\n",
        "# - Precision: Anteil der korrekt vorhergesagten Instanzen einer Klasse\n",
        "# - Recall: Anteil der tatsächlichen Instanzen einer Klasse, die korrekt erkannt wurden\n",
        "# - F1-Score: Harmonisches Mittel aus Precision und Recall\n",
        "# - Support: Anzahl der tatsächlichen Instanzen pro Klasse\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Classification Report:\n",
        "#                 precision    recall  f1-score   support\n",
        "#\n",
        "#      business       0.97      0.99      0.98       163\n",
        "# entertainment       0.99      0.78      0.87       120\n",
        "#      politics       0.86      0.99      0.92       112\n",
        "#         sport       0.91      1.00      0.95       148\n",
        "#          tech       1.00      0.93      0.96       125\n",
        "#\n",
        "#      accuracy                           0.94       668\n",
        "#     macro avg       0.95      0.94      0.94       668\n",
        "#  weighted avg       0.95      0.94      0.94       668\n",
        "\n",
        "\n",
        "######################################## Random Forest\n",
        "\n",
        "# Ein Random Forest ist ein Ensemble-Learning-Algorithmus,\n",
        "# der auf einer Sammlung von Entscheidungsbäumen basiert.\n",
        "# Es kombiniert mehrere Entscheidungsbäume,\n",
        "# um die Vorhersagegenauigkeit zu erhöhen und die Überanpassung (Overfitting) zu reduzieren.\n",
        "\n",
        "# Ein Random Forest ist ein Ensemble-Learning-Algorithmus,\n",
        "# der auf einer Sammlung von Entscheidungsbäumen basiert.\n",
        "# Es kombiniert mehrere Entscheidungsbäume, um die Vorhersagegenauigkeit\n",
        "# zu erhöhen und die Überanpassung (Overfitting) zu reduzieren.\n",
        "# Hier erkläre ich, wie und warum man Random Forest verwendet und wie\n",
        "# er sich von einem einzelnen Decision Tree unterscheidet.\n",
        "# 1. Was ist ein Decision Tree?\n",
        "# Ein Entscheidungsbaum (Decision Tree) ist ein baumbasiertes Modell,\n",
        "# das auf einer Hierarchie von Entscheidungen basiert.\n",
        "# Es teilt die Eingabedaten auf Basis von Merkmalen und Schwellenwerten\n",
        "# in rekursiven Schritten auf, bis eine Klassifikation (bei Klassifikationsproblemen)\n",
        "# oder ein Wert (bei Regressionsproblemen) erreicht ist.\n",
        "# Vorteile eines Entscheidungsbaums:\n",
        "# Einfach zu verstehen und zu interpretieren.\n",
        "# Kann sowohl für Klassifikation als auch Regression verwendet werden.\n",
        "# Benötigt wenig Datenvorbereitung (keine Normierung oder Skalierung).\n",
        "# Nachteile eines Entscheidungsbaums:\n",
        "# Überanpassung (Overfitting): Ein einzelner Baum neigt dazu,\n",
        "# sich zu sehr an die Trainingsdaten anzupassen.\n",
        "# Sensitivität gegenüber Rauschen: Kleine Änderungen in den Daten können die\n",
        "# Struktur des Baums drastisch verändern.\n",
        "# Ein Random Forest ist ein Ensemble-Learning-Algorithmus, der auf einer Sammlung von\n",
        "# Entscheidungsbäumen basiert. Es kombiniert mehrere Entscheidungsbäume,\n",
        "# um die Vorhersagegenauigkeit zu erhöhen und die Überanpassung (Overfitting) zu reduzieren.\n",
        "# Hier erkläre ich, wie und warum man Random Forest verwendet und wie er sich von einem\n",
        "# einzelnen Decision Tree unterscheidet.\n",
        "# 1. Was ist ein Decision Tree?\n",
        "# Ein Entscheidungsbaum (Decision Tree) ist ein baumbasiertes Modell, das auf einer\n",
        "# Hierarchie von Entscheidungen basiert. Es teilt die Eingabedaten auf Basis von\n",
        "# Merkmalen und Schwellenwerten in rekursiven Schritten auf, bis eine Klassifikation\n",
        "#  (bei Klassifikationsproblemen) oder ein Wert (bei Regressionsproblemen) erreicht ist.\n",
        "# Vorteile eines Entscheidungsbaums:\n",
        "# Einfach zu verstehen und zu interpretieren.\n",
        "# Kann sowohl für Klassifikation als auch Regression verwendet werden.\n",
        "# Benötigt wenig Datenvorbereitung (keine Normierung oder Skalierung).\n",
        "# Nachteile eines Entscheidungsbaums:\n",
        "# Überanpassung (Overfitting): Ein einzelner Baum neigt dazu, sich zu sehr an die\n",
        "# Trainingsdaten anzupassen.\n",
        "# Sensitivität gegenüber Rauschen: Kleine Änderungen in den Daten können die Struktur\n",
        "# des Baums drastisch verändern.\n",
        "# 2. Was ist ein Random Forest?\n",
        "# Random Forest ist ein Ensemble-Modell, das die Vorhersagen mehrerer Entscheidungsbäume\n",
        "# kombiniert. Es verwendet zwei Schlüsseltechniken, um die Leistung zu verbessern:\n",
        "# Bagging (Bootstrap Aggregation):\n",
        "# Für jeden Baum wird ein zufälliges Subset der Trainingsdaten (mit Zurücklegen) verwendet.\n",
        "# Dies erzeugt verschiedene Trainingssätze für die einzelnen Bäume, was die Varianz reduziert.\n",
        "# Feature Randomness:\n",
        "# Bei jedem Split eines Baums wird nicht auf alle Merkmale geschaut,\n",
        "# sondern nur auf eine zufällige Teilmenge der Merkmale.\n",
        "# Dies verhindert, dass ein Merkmal in allen Bäumen dominiert,\n",
        "# und sorgt für mehr Diversität.\n",
        "# Ablauf:\n",
        "# Erstelle n Entscheidungsbäume, wobei jeder Baum mit einem zufälligen Subset der\n",
        "# Trainingsdaten trainiert wird.\n",
        "# Für eine Klassifikation: Lasse jeden Baum eine Klasse vorhersagen,\n",
        "# und nimm die Mehrheit der Vorhersagen (Mehrheitsentscheidung).\n",
        "# Für eine Regression: Nimm den Durchschnitt der Vorhersagen aller Bäume.\n",
        "# 3. Warum verwendet man Random Forest?\n",
        "# Vorteile:\n",
        "# Robustheit gegen Overfitting: Da Random Forest auf der Aggregation vieler Bäume basiert,\n",
        "# ist es weniger wahrscheinlich, dass das Modell die Trainingsdaten überanpasst.\n",
        "# Stabilität: Es ist weniger anfällig für kleine Änderungen in den Trainingsdaten.\n",
        "# Automatische Feature-Auswahl: Da bei jedem Split nur eine Teilmenge von Merkmalen\n",
        "# betrachtet wird, werden irrelevante Merkmale oft ignoriert.\n",
        "# Flexibilität: Es kann sowohl für Klassifikations- als auch für Regressionsprobleme\n",
        "# eingesetzt werden.\n",
        "# Skalierbarkeit: Es kann parallelisiert werden, da jeder Baum unabhängig von den\n",
        "# anderen trainiert wird.\n",
        "# Nachteile:\n",
        "# Erhöhte Komplexität: Random Forest ist schwerer zu interpretieren als ein einzelner\n",
        "# Entscheidungsbaum.\n",
        "# Längere Trainingszeit: Da viele Bäume trainiert werden müssen, dauert das Training länger.\n",
        "# Speicherbedarf: Die Speicherung vieler Entscheidungsbäume benötigt mehr Speicherplatz.\n",
        "# Zusammenfassung\n",
        "# Decision Tree: Einfach, leicht zu interpretieren, aber anfällig für Overfitting.\n",
        "# Random Forest: Ein leistungsstarker, stabiler Algorithmus, der mehrere Entscheidungsbäume\n",
        "# kombiniert, um Overfitting zu reduzieren und die Genauigkeit zu verbessern.\n",
        "# Verwendung: Random Forest ist ideal für größere Datensätze mit vielen Features und in\n",
        "# Szenarien, in denen Genauigkeit wichtiger ist als Interpretierbarkeit.\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Zugriff auf Google Drive herstellen\n",
        "# Dieser Schritt ist erforderlich, um auf Dateien im Drive zuzugreifen (nur relevant in Google Colab).\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')  # Mountet das Google Drive und ermöglicht Zugriff auf Dateien.\n",
        "\n",
        "# Importieren der Pandas-Bibliothek\n",
        "import pandas as pd  # Pandas wird verwendet, um Daten in Tabellenform zu laden und zu manipulieren.\n",
        "\n",
        "# Laden der BBC-Daten aus einer CSV-Datei\n",
        "# Die CSV-Datei enthält zwei Spalten: 'text' (Artikeltexte) und 'class' (Kategorien der Artikel).\n",
        "df = pd.read_csv('/content/drive/My Drive/bbc_data.csv')\n",
        "\n",
        "# Anzeige der ersten Zeilen des DataFrames\n",
        "# Ermöglicht einen schnellen Überblick über die Struktur und den Inhalt der Daten.\n",
        "df.head()\n",
        "\n",
        "# Definieren der Eingabedaten (X) und Zielklassen (y)\n",
        "# X: Enthält die Texte (Artikelinhalte); y: Enthält die zugehörigen Kategorien.\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testdatensätze\n",
        "# 70% der Daten werden für das Training verwendet, 30% für das Testen.\n",
        "# random_state sorgt dafür, dass die Aufteilung reproduzierbar ist.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialisierung des CountVectorizers\n",
        "# CountVectorizer wandelt die Texte in numerische Merkmale um, basierend auf der Häufigkeit der Wörter.\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Anpassung und Transformation der Trainingsdaten\n",
        "# fit_transform: Lernt die Wort-Häufigkeiten und wandelt die Texte in numerische Merkmale um.\n",
        "X_train_vectorized = cv.fit_transform(X_train)\n",
        "\n",
        "# Transformation der Testdaten\n",
        "# transform: Wandelt die Testdaten in numerische Merkmale um, basierend auf den aus den Trainingsdaten gelernten Wort-Häufigkeiten.\n",
        "X_test_vectorized = cv.transform(X_test)\n",
        "\n",
        "# Initialisierung des Random Forest Klassifikators\n",
        "# RandomForestClassifier ist ein Ensemble-Modell, das mehrere Entscheidungsbäume kombiniert, um die Vorhersagegenauigkeit zu verbessern.\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "# Training des Random Forest Modells mit den vektorisierten Trainingsdaten\n",
        "classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Vorhersagen auf den Testdaten\n",
        "# Das trainierte Modell wird verwendet, um die Kategorien der Testdaten vorherzusagen.\n",
        "y_pred = classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Berechnung der Genauigkeit auf den Testdaten\n",
        "# accuracy_score misst den Anteil korrekt klassifizierter Datenpunkte.\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred)) # Test accuracy: 0.9640718562874252\n",
        "\n",
        "# Vorhersagen auf den Trainingsdaten\n",
        "# Überprüfung, ob das Modell überangepasst ist (Overfitting), indem die Trainingsgenauigkeit berechnet wird.\n",
        "y_pred = classifier.predict(X_train_vectorized)\n",
        "\n",
        "# Berechnung der Genauigkeit auf den Trainingsdaten\n",
        "print(\"Train accuracy:\", accuracy_score(y_train, y_pred)) # Train accuracy: 1.0\n",
        "\n",
        "\n",
        "\n",
        "#################################### Random Forest best hyperparameters\n",
        "\n",
        "# Hyperparameter sind die Einstellungen eines Modells, die nicht während des Trainings gelernt werden,\n",
        "# sondern vom Benutzer festgelegt werden müssen. Beispiele:\n",
        "# Anzahl der Bäume in einem Random Forest.\n",
        "# Tiefe eines Entscheidungsbaums.\n",
        "# Lernrate bei Gradientenabstiegsalgorithmen.\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Zugriff auf Google Drive herstellen\n",
        "# Dieser Schritt ist erforderlich, wenn mit Google Colab gearbeitet wird, um auf Dateien im Drive zuzugreifen.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importieren der Pandas-Bibliothek\n",
        "import pandas as pd  # Ermöglicht die Arbeit mit Daten in Tabellenform\n",
        "\n",
        "# Laden der BBC-Daten aus einer CSV-Datei\n",
        "# Die Datei enthält zwei Spalten: 'text' (Artikeltexte) und 'class' (Kategorien der Artikel).\n",
        "df = pd.read_csv('/content/drive/My Drive/bbc_data.csv')\n",
        "\n",
        "# Definieren der Eingabedaten (X) und Zielklassen (y)\n",
        "# X enthält die Texte der Artikel, y enthält die zugehörigen Kategorien.\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testdaten\n",
        "# Trainingsdaten: 70% der Daten, Testdaten: 30% der Daten.\n",
        "# random_state sorgt dafür, dass die Aufteilung reproduzierbar ist.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialisierung des CountVectorizers\n",
        "# CountVectorizer wandelt die Textdaten in numerische Merkmale um, basierend auf der Häufigkeit der Wörter.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Anpassung und Transformation der Trainingsdaten\n",
        "# fit_transform lernt die Wort-Häufigkeiten und transformiert die Texte in numerische Merkmale.\n",
        "X_train_vectorized = cv.fit_transform(X_train)\n",
        "\n",
        "# Transformation der Testdaten\n",
        "# transform nutzt die Wort-Häufigkeiten, die aus den Trainingsdaten gelernt wurden, um die Testdaten zu transformieren.\n",
        "X_test_vectorized = cv.transform(X_test)\n",
        "\n",
        "# Initialisierung des Random Forest Klassifikators mit spezifischen Hyperparametern\n",
        "# max_features: Maximale Anzahl von Merkmalen, die für die besten Splits in Betracht gezogen werden.\n",
        "# min_samples_leaf: Minimale Anzahl von Samples, die in einem Blatt-Knoten vorhanden sein müssen.\n",
        "# min_samples_split: Minimale Anzahl von Samples, die erforderlich sind, um einen Knoten zu teilen.\n",
        "# n_estimators: Anzahl der Entscheidungsbäume im Wald.\n",
        "# classifier = RandomForestClassifier(max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=300)\n",
        "# Best hyperparameters are {'max_features': 30, 'min_samples_leaf': 4, 'min_samples_split': 6, 'n_estimators': 300}\n",
        "classifier =RandomForestClassifier(max_features = 30, min_samples_leaf = 4, min_samples_split = 6, n_estimators = 300)\n",
        "\n",
        "\n",
        "# Training des Random Forest Modells\n",
        "# Das Modell wird mit den vektorisierten Trainingsdaten (X_train_vectorized) und den Zielklassen (y_train) trainiert.\n",
        "classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Vorhersagen auf den Testdaten\n",
        "# Das trainierte Modell wird verwendet, um die Kategorien der Testdaten vorherzusagen.\n",
        "y_pred = classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Berechnung und Ausgabe der Genauigkeit auf den Testdaten\n",
        "# accuracy_score misst den Anteil korrekt klassifizierter Datenpunkte.\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred)) # Test accuracy: 0.9416167664670658\n",
        "\n",
        "# Vorhersagen auf den Trainingsdaten\n",
        "# Dies dient zur Überprüfung, ob das Modell überangepasst ist (Overfitting).\n",
        "y_pred = classifier.predict(X_train_vectorized)\n",
        "\n",
        "# Berechnung und Ausgabe der Genauigkeit auf den Trainingsdaten\n",
        "print(\"Train accuracy:\", accuracy_score(y_train, y_pred)) # Train accuracy: 0.9736673089274245\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## Random Forest Grid search\n",
        "\n",
        "# Bei Grid Search wird ein Suchraum von Hyperparametern definiert. Dabei werden alle möglichen Kombinationen ausprobiert,\n",
        "# um die beste Kombination für das Modell zu finden.\n",
        "# Mit Grid Search werden systematisch die besten Parameter gesucht, die das Modell am besten auf die Daten abstimmen.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Zugriff auf Google Drive herstellen\n",
        "# Dieser Schritt ermöglicht den Zugriff auf Dateien im Drive, wenn Google Colab verwendet wird.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')  # Mountet das Google Drive und stellt den Zugriff auf Dateien sicher.\n",
        "\n",
        "# Importieren der Pandas-Bibliothek\n",
        "import pandas as pd  # Pandas wird verwendet, um Daten in Tabellenform zu laden und zu verarbeiten.\n",
        "\n",
        "# Laden der BBC-Daten aus einer CSV-Datei\n",
        "# Die Datei enthält zwei Spalten: 'text' (Artikeltexte) und 'class' (Kategorien der Artikel).\n",
        "df = pd.read_csv('/content/drive/My Drive/bbc_data.csv')\n",
        "\n",
        "# Definieren der Eingabedaten (X) und Zielklassen (y)\n",
        "# X: Enthält die Artikeltexte, y: Enthält die zugehörigen Kategorien.\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Aufteilen der Daten in Trainings- und Testdatensätze\n",
        "# 70% der Daten werden für das Training verwendet, 30% für das Testen.\n",
        "# random_state sorgt dafür, dass die Aufteilung reproduzierbar ist.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialisierung des CountVectorizers\n",
        "# CountVectorizer wandelt die Texte in numerische Merkmale um, basierend auf der Häufigkeit der Wörter.\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Anpassung und Transformation der Trainingsdaten\n",
        "# fit_transform: Lernt die Wort-Häufigkeiten und wandelt die Texte in numerische Merkmale um.\n",
        "X_train_vectorized = cv.fit_transform(X_train)\n",
        "\n",
        "# Transformation der Testdaten\n",
        "# transform: Wandelt die Testdaten in numerische Merkmale um, basierend auf den aus den Trainingsdaten gelernten Wort-Häufigkeiten.\n",
        "X_test_vectorized = cv.transform(X_test)\n",
        "\n",
        "# Initialisierung und Training des Random Forest Klassifikators\n",
        "# RandomForestClassifier ist ein Ensemble-Modell, das mehrere Entscheidungsbäume kombiniert, um die Vorhersagegenauigkeit zu verbessern.\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train_vectorized, y_train)  # Anpassen des Modells an die Trainingsdaten\n",
        "\n",
        "# Vorhersagen auf den Testdaten\n",
        "# Das trainierte Modell wird verwendet, um die Kategorien der Testdaten vorherzusagen.\n",
        "y_pred = classifier.predict(X_test_vectorized)\n",
        "\n",
        "# Berechnung und Ausgabe der Genauigkeit auf den Testdaten\n",
        "# accuracy_score misst den Anteil korrekt klassifizierter Datenpunkte.\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred)) # Test accuracy: 0.9565868263473054\n",
        "\n",
        "# Vorhersagen auf den Trainingsdaten\n",
        "# Überprüfung, ob das Modell überangepasst ist (Overfitting), indem die Trainingsgenauigkeit berechnet wird.\n",
        "y_pred = classifier.predict(X_train_vectorized)\n",
        "\n",
        "# Berechnung und Ausgabe der Genauigkeit auf den Trainingsdaten\n",
        "print(\"Train accuracy:\", accuracy_score(y_train, y_pred)) # Train accuracy: 1.0\n",
        "\n",
        "# Hyperparameter-Optimierung mit GridSearchCV\n",
        "# GridSearchCV durchsucht systematisch alle Kombinationen der angegebenen Hyperparameter.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definition der Hyperparameter, die optimiert werden sollen\n",
        "params = {\n",
        "   'n_estimators': [100, 200, 300],  # Anzahl der Entscheidungsbäume im Wald\n",
        "   'max_features': [10, 20, 30],    # Maximale Anzahl von Merkmalen, die für die besten Splits in Betracht gezogen werden\n",
        "   'min_samples_split': [4, 6, 8],  # Minimale Anzahl von Samples, die erforderlich sind, um einen Knoten zu teilen\n",
        "   'min_samples_leaf': [4, 6, 8]    # Minimale Anzahl von Samples, die in einem Blatt-Knoten vorhanden sein müssen\n",
        "}\n",
        "\n",
        "# Initialisierung von GridSearchCV\n",
        "# cv=5 bedeutet, dass eine 5-fache Kreuzvalidierung verwendet wird.\n",
        "# cv = cross validation, Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
        "# verbose=3 sorgt für detaillierte Ausgaben über den Fortschritt der Optimierung.\n",
        "grid = GridSearchCV(classifier, params, cv=5, verbose=3)\n",
        "\n",
        "# Training des Modells mit den besten Hyperparametern\n",
        "# fit() passt das Modell an die Trainingsdaten an und bewertet die verschiedenen Parameterkombinationen.\n",
        "model = grid.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Ausgabe der besten Hyperparameterkombination\n",
        "# best_params_ gibt die Parameterkombination mit der höchsten Leistung zurück.\n",
        "print('Best hyperparameters are ' + str(model.best_params_))\n",
        "# Best hyperparameters are {'max_features': 30, 'min_samples_leaf': 4, 'min_samples_split': 6, 'n_estimators': 300}\n",
        "\n",
        "# Ausgabe des besten Scores der Kreuzvalidierung\n",
        "# best_score_ gibt die höchste Genauigkeit über die Kreuzvalidierungsfalten hinweg an.\n",
        "print('Best score is: ' + str(model.best_score_))\n",
        "# Best score is: 0.9293573254184186\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10035
        },
        "id": "DnmIrfHU3-7s",
        "outputId": "3c3af8c6-d75f-45ce-b31d-b76a641d61a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "  (0, 7)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 0)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 4)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 6)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 1)\t1\n",
            "[[1 0 1 0 0 1 0 1 0 0]\n",
            " [0 0 0 1 1 0 0 1 0 0]\n",
            " [1 1 0 0 0 1 1 0 1 1]]\n",
            "   bag  buy  is  like  movie  my  she  this  to  wants\n",
            "0    1    0   1     0      0   1    0     1   0      0\n",
            "1    0    0   0     1      1   0    0     1   0      0\n",
            "2    1    1   0     0      0   1    1     0   1      1\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                                text          class\n",
            "0  Musicians to tackle US red tape  Musicians gro...  entertainment\n",
            "1  U2s desire to be number one  U2, who have won ...  entertainment\n",
            "2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment\n",
            "3  Snicket tops US box office chart  The film ada...  entertainment\n",
            "4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment\n",
            "(2225, 2)\n",
            "Accuracy: 0.9790419161676647\n",
            "[[157   0   2   0   4]\n",
            " [  1 117   1   0   1]\n",
            " [  1   0 111   0   0]\n",
            " [  1   0   0 147   0]\n",
            " [  1   1   1   0 122]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"bbd0fe69-da72-4a97-9fb4-f94ca7cca38c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bbd0fe69-da72-4a97-9fb4-f94ca7cca38c\")) {                    Plotly.newPlot(                        \"bbd0fe69-da72-4a97-9fb4-f94ca7cca38c\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"],\"y\":[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"],\"z\":[[157,0,2,0,4],[1,117,1,0,1],[1,0,111,0,0],[1,0,0,147,0],[1,1,1,0,122]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Vorhergesagte Werte: %{x}\\u003cbr\\u003eTatsächliche Werte: %{y}\\u003cbr\\u003eHäufigkeit: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"text\":[[157,0,2,0,4],[1,117,1,0,1],[1,0,111,0,0],[1,0,0,147,0],[1,1,1,0,122]],\"texttemplate\":\"%{text}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Vorhergesagte Werte\"},\"side\":\"bottom\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Tatsächliche Werte\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Häufigkeit\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bbd0fe69-da72-4a97-9fb4-f94ca7cca38c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     business       0.98      0.96      0.97       163\n",
            "entertainment       0.99      0.97      0.98       120\n",
            "     politics       0.97      0.99      0.98       112\n",
            "        sport       1.00      0.99      1.00       148\n",
            "         tech       0.96      0.98      0.97       125\n",
            "\n",
            "     accuracy                           0.98       668\n",
            "    macro avg       0.98      0.98      0.98       668\n",
            " weighted avg       0.98      0.98      0.98       668\n",
            "\n",
            "        and  document     first        is       one    second       the  \\\n",
            "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
            "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
            "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
            "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
            "\n",
            "      third      this  \n",
            "0  0.000000  0.384085  \n",
            "1  0.000000  0.281089  \n",
            "2  0.511849  0.267104  \n",
            "3  0.000000  0.384085  \n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Accuracy: 0.9431137724550899\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"0a9f6e70-dcaf-4b13-8308-d3d482cf2920\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0a9f6e70-dcaf-4b13-8308-d3d482cf2920\")) {                    Plotly.newPlot(                        \"0a9f6e70-dcaf-4b13-8308-d3d482cf2920\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"],\"y\":[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"],\"z\":[[161,0,2,0,0],[2,94,12,12,0],[1,0,111,0,0],[0,0,0,148,0],[2,1,4,2,116]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Vorhergesagte Werte: %{x}\\u003cbr\\u003eTatsächliche Werte: %{y}\\u003cbr\\u003eHäufigkeit: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"text\":[[161,0,2,0,0],[2,94,12,12,0],[1,0,111,0,0],[0,0,0,148,0],[2,1,4,2,116]],\"texttemplate\":\"%{text}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Vorhergesagte Werte\"},\"side\":\"bottom\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Tatsächliche Werte\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Häufigkeit\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0a9f6e70-dcaf-4b13-8308-d3d482cf2920');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     business       0.97      0.99      0.98       163\n",
            "entertainment       0.99      0.78      0.87       120\n",
            "     politics       0.86      0.99      0.92       112\n",
            "        sport       0.91      1.00      0.95       148\n",
            "         tech       1.00      0.93      0.96       125\n",
            "\n",
            "     accuracy                           0.94       668\n",
            "    macro avg       0.95      0.94      0.94       668\n",
            " weighted avg       0.95      0.94      0.94       668\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Test accuracy: 0.9565868263473054\n",
            "Train accuracy: 1.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Test accuracy: 0.938622754491018\n",
            "Train accuracy: 0.970456005138086\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Test accuracy: 0.9550898203592815\n",
            "Train accuracy: 1.0\n",
            "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.676 total time=   0.3s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.670 total time=   0.2s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.646 total time=   0.2s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.640 total time=   0.2s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.627 total time=   0.3s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.673 total time=   0.5s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.651 total time=   0.5s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.659 total time=   0.5s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.656 total time=   0.5s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.637 total time=   0.7s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.670 total time=   1.0s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.679 total time=   1.0s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.637 total time=   1.1s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.650 total time=   0.9s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.653 total time=   0.7s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.660 total time=   0.2s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.676 total time=   0.2s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.675 total time=   0.3s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.659 total time=   0.3s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.675 total time=   0.3s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.667 total time=   0.5s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.683 total time=   0.5s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.678 total time=   0.5s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.650 total time=   0.5s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.624 total time=   0.5s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.670 total time=   0.7s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.663 total time=   0.7s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.637 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.656 total time=   0.7s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.678 total time=   0.7s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.583 total time=   0.2s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.635 total time=   0.2s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.650 total time=   0.2s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.707 total time=   0.3s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.627 total time=   0.3s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.638 total time=   0.5s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.679 total time=   0.7s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.669 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.659 total time=   0.7s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.672 total time=   0.7s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.679 total time=   1.1s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.660 total time=   1.3s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.678 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.656 total time=   0.7s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.659 total time=   0.7s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.545 total time=   0.2s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.474 total time=   0.2s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.585 total time=   0.2s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.514 total time=   0.2s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.524 total time=   0.3s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.564 total time=   0.4s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.561 total time=   0.4s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.566 total time=   0.5s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.531 total time=   0.4s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.492 total time=   0.5s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.513 total time=   0.7s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.571 total time=   0.7s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.553 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.553 total time=   0.7s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.550 total time=   0.7s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.532 total time=   0.3s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.532 total time=   0.4s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.521 total time=   0.3s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.498 total time=   0.3s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.498 total time=   0.4s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.551 total time=   0.6s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.503 total time=   0.6s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.524 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.543 total time=   0.7s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.543 total time=   0.6s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.526 total time=   0.7s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.574 total time=   0.7s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.550 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.543 total time=   0.7s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.495 total time=   0.7s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.458 total time=   0.2s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.471 total time=   0.2s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.527 total time=   0.2s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.482 total time=   0.2s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.495 total time=   0.2s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.532 total time=   0.5s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.545 total time=   0.4s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.495 total time=   0.5s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.505 total time=   0.5s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.579 total time=   0.5s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.548 total time=   0.7s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.545 total time=   0.7s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.537 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.527 total time=   0.7s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.553 total time=   1.0s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.433 total time=   1.0s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.455 total time=   1.0s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.444 total time=   0.4s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.402 total time=   0.4s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.498 total time=   0.4s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.442 total time=   0.7s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.442 total time=   0.4s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.437 total time=   0.4s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.469 total time=   0.4s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.492 total time=   0.4s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.436 total time=   0.7s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.462 total time=   0.6s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.424 total time=   0.7s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.428 total time=   0.6s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.460 total time=   0.6s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.487 total time=   0.3s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.436 total time=   0.2s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.434 total time=   0.2s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.418 total time=   0.2s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.412 total time=   0.2s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.468 total time=   0.4s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.442 total time=   0.4s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.486 total time=   0.4s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.466 total time=   0.4s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.457 total time=   0.4s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.458 total time=   0.6s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.423 total time=   0.6s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.466 total time=   0.8s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.479 total time=   1.0s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.431 total time=   1.0s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.420 total time=   0.3s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.410 total time=   0.3s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.486 total time=   0.4s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.405 total time=   0.4s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.437 total time=   0.4s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.506 total time=   0.5s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.494 total time=   0.4s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.495 total time=   0.4s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.402 total time=   0.4s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.463 total time=   0.4s\n",
            "[CV 1/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.481 total time=   0.6s\n",
            "[CV 2/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.449 total time=   0.7s\n",
            "[CV 3/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.457 total time=   0.6s\n",
            "[CV 4/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.437 total time=   0.6s\n",
            "[CV 5/5] END max_features=10, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.457 total time=   0.6s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.837 total time=   0.3s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.865 total time=   0.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.881 total time=   0.3s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.910 total time=   0.3s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.900 total time=   0.3s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.888 total time=   0.6s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.878 total time=   0.6s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.900 total time=   0.6s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.900 total time=   0.6s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.868 total time=   0.8s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.891 total time=   1.4s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.888 total time=   1.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.884 total time=   1.4s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.897 total time=   1.0s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.913 total time=   0.9s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.875 total time=   0.3s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.862 total time=   0.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.894 total time=   0.3s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.900 total time=   0.3s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.884 total time=   0.3s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.878 total time=   0.6s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.881 total time=   0.6s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.884 total time=   0.7s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.904 total time=   0.6s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.884 total time=   0.7s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.894 total time=   0.9s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.891 total time=   1.0s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.878 total time=   0.9s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.913 total time=   1.1s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.904 total time=   1.4s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.869 total time=   0.5s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.878 total time=   0.5s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.868 total time=   0.5s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.910 total time=   0.5s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.881 total time=   0.5s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.885 total time=   0.6s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.878 total time=   0.6s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.878 total time=   0.6s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.897 total time=   0.7s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.891 total time=   0.6s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.904 total time=   1.0s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.878 total time=   1.0s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.897 total time=   0.9s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.887 total time=   0.9s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.907 total time=   0.9s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.798 total time=   0.3s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.782 total time=   0.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.836 total time=   0.3s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.791 total time=   0.3s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.804 total time=   0.5s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.814 total time=   0.9s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.785 total time=   1.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.842 total time=   1.2s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.797 total time=   1.2s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.797 total time=   1.1s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.798 total time=   1.4s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.808 total time=   1.1s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.804 total time=   1.2s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.820 total time=   0.8s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.823 total time=   0.8s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.811 total time=   0.3s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.782 total time=   0.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.801 total time=   0.3s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.804 total time=   0.3s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.785 total time=   0.3s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.846 total time=   0.6s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.801 total time=   0.6s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.846 total time=   0.6s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.820 total time=   0.6s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.830 total time=   0.5s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.785 total time=   0.8s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.782 total time=   1.0s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.836 total time=   1.2s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.814 total time=   1.1s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.836 total time=   1.2s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.779 total time=   0.4s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.808 total time=   0.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.810 total time=   0.3s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.801 total time=   0.3s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.833 total time=   0.3s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.830 total time=   0.5s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.798 total time=   0.5s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.807 total time=   0.5s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.842 total time=   0.6s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.842 total time=   0.6s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.811 total time=   0.8s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.811 total time=   0.8s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.833 total time=   0.8s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.836 total time=   0.8s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.842 total time=   0.8s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.744 total time=   0.3s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.747 total time=   0.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.736 total time=   0.3s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.749 total time=   0.3s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.775 total time=   0.3s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.728 total time=   0.5s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.699 total time=   0.7s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.765 total time=   0.7s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.720 total time=   0.7s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.727 total time=   0.7s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.769 total time=   1.1s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.721 total time=   1.0s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.736 total time=   0.7s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.746 total time=   0.7s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.730 total time=   0.7s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.740 total time=   0.3s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.721 total time=   0.3s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.727 total time=   0.3s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.736 total time=   0.3s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.698 total time=   0.3s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.708 total time=   0.5s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.753 total time=   0.5s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.714 total time=   0.5s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.720 total time=   0.5s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.765 total time=   0.5s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.763 total time=   0.8s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.763 total time=   0.7s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.781 total time=   0.8s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.746 total time=   0.7s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.759 total time=   0.8s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.737 total time=   0.4s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.747 total time=   0.4s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.730 total time=   0.4s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.743 total time=   0.4s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.762 total time=   0.4s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.753 total time=   0.7s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.747 total time=   0.8s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.727 total time=   0.7s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.714 total time=   0.7s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.749 total time=   0.5s\n",
            "[CV 1/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.715 total time=   0.7s\n",
            "[CV 2/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.744 total time=   0.7s\n",
            "[CV 3/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.743 total time=   0.8s\n",
            "[CV 4/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.730 total time=   0.7s\n",
            "[CV 5/5] END max_features=20, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.727 total time=   0.8s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.907 total time=   0.4s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.904 total time=   0.4s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.910 total time=   0.4s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.920 total time=   0.4s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=100;, score=0.923 total time=   0.4s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.920 total time=   0.7s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.894 total time=   0.7s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.929 total time=   0.7s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.926 total time=   0.7s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=200;, score=0.942 total time=   0.8s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.929 total time=   1.6s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.920 total time=   1.6s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.923 total time=   1.6s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.945 total time=   1.1s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=4, n_estimators=300;, score=0.926 total time=   1.1s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.904 total time=   0.4s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.904 total time=   0.4s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.920 total time=   0.4s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.936 total time=   0.4s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=100;, score=0.926 total time=   0.4s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.926 total time=   0.7s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.907 total time=   0.7s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.913 total time=   0.7s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.913 total time=   0.8s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=200;, score=0.936 total time=   0.8s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.933 total time=   1.1s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.917 total time=   1.2s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.904 total time=   1.6s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.939 total time=   1.6s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=6, n_estimators=300;, score=0.926 total time=   1.4s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.939 total time=   0.4s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.926 total time=   0.4s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.923 total time=   0.4s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.929 total time=   0.4s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=100;, score=0.929 total time=   0.4s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.923 total time=   0.8s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.923 total time=   0.7s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.929 total time=   0.7s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.939 total time=   0.7s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=200;, score=0.929 total time=   0.7s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.920 total time=   1.1s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.917 total time=   1.1s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.916 total time=   1.1s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.932 total time=   1.3s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=4, min_samples_split=8, n_estimators=300;, score=0.923 total time=   1.6s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.875 total time=   0.5s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.885 total time=   0.5s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.894 total time=   0.5s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.920 total time=   0.6s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=100;, score=0.884 total time=   0.3s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.878 total time=   0.7s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.885 total time=   0.6s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.868 total time=   0.7s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.900 total time=   0.6s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=200;, score=0.900 total time=   0.6s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.923 total time=   0.9s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.865 total time=   1.0s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.891 total time=   1.0s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.916 total time=   1.0s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=4, n_estimators=300;, score=0.884 total time=   1.0s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.891 total time=   0.3s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.869 total time=   0.3s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.897 total time=   0.3s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.904 total time=   0.3s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=100;, score=0.891 total time=   0.4s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.888 total time=   0.9s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.891 total time=   0.9s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.887 total time=   1.0s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.910 total time=   1.0s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=200;, score=0.916 total time=   0.8s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.894 total time=   0.9s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.888 total time=   0.9s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.894 total time=   0.9s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.916 total time=   0.9s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=6, n_estimators=300;, score=0.894 total time=   0.9s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.862 total time=   0.3s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.869 total time=   0.3s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.878 total time=   0.3s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.900 total time=   0.3s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=100;, score=0.878 total time=   0.3s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.901 total time=   0.6s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.881 total time=   0.6s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.881 total time=   0.6s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.904 total time=   0.6s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=200;, score=0.891 total time=   0.6s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.901 total time=   1.4s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.875 total time=   1.4s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.916 total time=   1.5s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.913 total time=   1.0s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=6, min_samples_split=8, n_estimators=300;, score=0.897 total time=   0.9s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.865 total time=   0.3s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.827 total time=   0.3s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.859 total time=   0.3s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.849 total time=   0.3s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=100;, score=0.865 total time=   0.3s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.872 total time=   0.6s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.837 total time=   0.6s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.846 total time=   0.6s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.907 total time=   0.6s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=200;, score=0.894 total time=   0.6s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.853 total time=   0.9s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.830 total time=   0.9s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.855 total time=   0.9s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.891 total time=   0.9s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=4, n_estimators=300;, score=0.849 total time=   1.2s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.824 total time=   0.4s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.808 total time=   0.5s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.855 total time=   0.4s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.859 total time=   0.4s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=100;, score=0.842 total time=   0.5s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.843 total time=   0.9s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.827 total time=   0.7s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.839 total time=   0.6s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.859 total time=   0.6s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=200;, score=0.852 total time=   0.6s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.833 total time=   0.9s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.821 total time=   0.9s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.859 total time=   0.9s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.875 total time=   0.9s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=6, n_estimators=300;, score=0.871 total time=   0.9s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.843 total time=   0.3s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.830 total time=   0.3s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.846 total time=   0.3s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.849 total time=   0.3s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=100;, score=0.865 total time=   0.3s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.869 total time=   0.6s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.853 total time=   0.6s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.878 total time=   0.6s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.878 total time=   0.7s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=200;, score=0.839 total time=   0.8s\n",
            "[CV 1/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.843 total time=   1.2s\n",
            "[CV 2/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.837 total time=   1.3s\n",
            "[CV 3/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.836 total time=   1.1s\n",
            "[CV 4/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.859 total time=   0.9s\n",
            "[CV 5/5] END max_features=30, min_samples_leaf=8, min_samples_split=8, n_estimators=300;, score=0.855 total time=   0.9s\n",
            "Best hyperparameters are {'max_features': 30, 'min_samples_leaf': 4, 'min_samples_split': 8, 'n_estimators': 100}\n",
            "Best score is: 0.9293470195399456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "naive Bayes with grid search"
      ],
      "metadata": {
        "id": "w7_Vhqt58T2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/bbc_data.csv')\n",
        "\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "random_state=42)\n",
        "cv = CountVectorizer()# create an instance of the class CountVectorizer\n",
        "X_train_vectorized = cv.fit_transform(X_train)\n",
        "X_test_vectorized = cv.transform(X_test)\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_vectorized, y_train)\n",
        "params= {\n",
        " 'alpha': [0.1, 0.5, 1.0, 2.0],\n",
        " 'fit_prior': [True, False]\n",
        "}\n",
        "grid = GridSearchCV(classifier, params, cv = 5, verbose = 3)\n",
        "model = grid.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Ausgabe der besten Hyperparameterkombination\n",
        "# best_params_ gibt die Parameterkombination mit der höchsten Leistung zurück.\n",
        "print('Best hyperparameters are '+str(model.best_params_))\n",
        "# Best hyperparameters are {'alpha': 0.1, 'fit_prior': True}\n",
        "\n",
        "# Ausgabe des besten Scores der Kreuzvalidierung\n",
        "# best_score_ gibt die höchste Genauigkeit über die Kreuzvalidierungsfalten hinweg an.\n",
        "print('Best score is: ' + str(model.best_score_))\n",
        "# Best score is: 0.9736746640283618\n",
        "\n",
        "y_pred = classifier.predict(X_test_vectorized)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
        "# Test accuracy: 0.9790419161676647\n",
        "\n",
        "y_pred = classifier.predict(X_train_vectorized)\n",
        "\n",
        "print(\"Train accuracy:\", accuracy_score(y_train, y_pred))\n",
        "# Train accuracy: 0.9942196531791907"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgh3h3Gh8YgY",
        "outputId": "33da3329-a0cd-4c7b-de67-5e1fdf9e5463"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "[CV 1/5] END .........alpha=0.1, fit_prior=True;, score=0.962 total time=   0.0s\n",
            "[CV 2/5] END .........alpha=0.1, fit_prior=True;, score=0.974 total time=   0.0s\n",
            "[CV 3/5] END .........alpha=0.1, fit_prior=True;, score=0.974 total time=   0.0s\n",
            "[CV 4/5] END .........alpha=0.1, fit_prior=True;, score=0.977 total time=   0.0s\n",
            "[CV 5/5] END .........alpha=0.1, fit_prior=True;, score=0.981 total time=   0.0s\n",
            "[CV 1/5] END ........alpha=0.1, fit_prior=False;, score=0.962 total time=   0.0s\n",
            "[CV 2/5] END ........alpha=0.1, fit_prior=False;, score=0.974 total time=   0.0s\n",
            "[CV 3/5] END ........alpha=0.1, fit_prior=False;, score=0.974 total time=   0.0s\n",
            "[CV 4/5] END ........alpha=0.1, fit_prior=False;, score=0.977 total time=   0.0s\n",
            "[CV 5/5] END ........alpha=0.1, fit_prior=False;, score=0.981 total time=   0.0s\n",
            "[CV 1/5] END .........alpha=0.5, fit_prior=True;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END .........alpha=0.5, fit_prior=True;, score=0.974 total time=   0.0s\n",
            "[CV 3/5] END .........alpha=0.5, fit_prior=True;, score=0.965 total time=   0.0s\n",
            "[CV 4/5] END .........alpha=0.5, fit_prior=True;, score=0.977 total time=   0.0s\n",
            "[CV 5/5] END .........alpha=0.5, fit_prior=True;, score=0.974 total time=   0.0s\n",
            "[CV 1/5] END ........alpha=0.5, fit_prior=False;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END ........alpha=0.5, fit_prior=False;, score=0.974 total time=   0.0s\n",
            "[CV 3/5] END ........alpha=0.5, fit_prior=False;, score=0.965 total time=   0.0s\n",
            "[CV 4/5] END ........alpha=0.5, fit_prior=False;, score=0.977 total time=   0.0s\n",
            "[CV 5/5] END ........alpha=0.5, fit_prior=False;, score=0.974 total time=   0.0s\n",
            "[CV 1/5] END .........alpha=1.0, fit_prior=True;, score=0.949 total time=   0.0s\n",
            "[CV 2/5] END .........alpha=1.0, fit_prior=True;, score=0.971 total time=   0.0s\n",
            "[CV 3/5] END .........alpha=1.0, fit_prior=True;, score=0.965 total time=   0.0s\n",
            "[CV 4/5] END .........alpha=1.0, fit_prior=True;, score=0.971 total time=   0.0s\n",
            "[CV 5/5] END .........alpha=1.0, fit_prior=True;, score=0.971 total time=   0.0s\n",
            "[CV 1/5] END ........alpha=1.0, fit_prior=False;, score=0.949 total time=   0.0s\n",
            "[CV 2/5] END ........alpha=1.0, fit_prior=False;, score=0.971 total time=   0.0s\n",
            "[CV 3/5] END ........alpha=1.0, fit_prior=False;, score=0.965 total time=   0.0s\n",
            "[CV 4/5] END ........alpha=1.0, fit_prior=False;, score=0.971 total time=   0.0s\n",
            "[CV 5/5] END ........alpha=1.0, fit_prior=False;, score=0.971 total time=   0.0s\n",
            "[CV 1/5] END .........alpha=2.0, fit_prior=True;, score=0.936 total time=   0.0s\n",
            "[CV 2/5] END .........alpha=2.0, fit_prior=True;, score=0.958 total time=   0.0s\n",
            "[CV 3/5] END .........alpha=2.0, fit_prior=True;, score=0.965 total time=   0.0s\n",
            "[CV 4/5] END .........alpha=2.0, fit_prior=True;, score=0.961 total time=   0.0s\n",
            "[CV 5/5] END .........alpha=2.0, fit_prior=True;, score=0.958 total time=   0.0s\n",
            "[CV 1/5] END ........alpha=2.0, fit_prior=False;, score=0.936 total time=   0.0s\n",
            "[CV 2/5] END ........alpha=2.0, fit_prior=False;, score=0.958 total time=   0.0s\n",
            "[CV 3/5] END ........alpha=2.0, fit_prior=False;, score=0.965 total time=   0.0s\n",
            "[CV 4/5] END ........alpha=2.0, fit_prior=False;, score=0.961 total time=   0.0s\n",
            "[CV 5/5] END ........alpha=2.0, fit_prior=False;, score=0.958 total time=   0.0s\n",
            "Best hyperparameters are {'alpha': 0.1, 'fit_prior': True}\n",
            "Best score is: 0.9736746640283618\n",
            "Best hyperparameters are {'alpha': 0.1, 'fit_prior': True}\n"
          ]
        }
      ]
    }
  ]
}