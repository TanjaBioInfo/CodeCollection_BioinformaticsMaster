{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0b4795",
   "metadata": {},
   "source": [
    "## Zusammenfassung: PDF- und HTML-Scraping\n",
    "Dieses Notebook demonstriert anhand von Beispielen, wie man:\n",
    "1. **PDF-Scraping** durchführt, um Inhalte aus PDFs zu extrahieren.\n",
    "2. **HTML-Scraping** verwendet, um Daten aus Webseiten zu extrahieren.\n",
    "3. Die extrahierten Daten speichert und weiterverarbeitet.\n",
    "Jeder Schritt ist mit Kommentaren und Erläuterungen auf Deutsch versehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f0fc9",
   "metadata": {},
   "source": [
    "## Combined Notebook with Annotations\n",
    "This notebook merges and annotates the content from all uploaded notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06917002",
   "metadata": {},
   "source": [
    "#### Content from: Excercise 2. Modelling the Weighted Social Network of Hamlet..ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f954d20",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40175941",
   "metadata": {},
   "source": [
    "Let's practice what we have just learnt! Let's follow this article (https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf) and built the social network of **Hamlet** by William Shakespeare. We will be using this version of the book: https://www.gutenberg.org/files/1524/1524-h/1524-h.htm\n",
    "\n",
    "**Note**: we are not using *Vingt mille lieues sous les mers* because a) it is a super long book (300+ pages) plus it has not so many dialogues, so it may not be the best study case to work with Social Networks considering the amount of time that we have. However, Network Analysis with NetworkX can be done multilingually!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4965d",
   "metadata": {},
   "source": [
    "# 1. First we import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93463a",
   "metadata": {},
   "source": [
    "# 2. The we create the G object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2956d78",
   "metadata": {},
   "source": [
    "Just to let you know with this command we can clean our network (for example if we make a spelling mistake that contaminates our Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e381aa",
   "metadata": {},
   "source": [
    "# 3. Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546d930",
   "metadata": {},
   "source": [
    "Now we transform every character into a node by writing each name inside **G.add_node()**. Only the main characters are included in here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a886b",
   "metadata": {},
   "source": [
    "These are the play characters (you can find this information at the beginning of the book). Remember to change \"Claudius\" for \"King\" and \"Gertrude\" for \"Queen\" as that is how they will appear throughout the play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da98594",
   "metadata": {},
   "source": [
    "Dramatis Personæ\n",
    "\n",
    "* HAMLET, Prince of Denmark\n",
    "* CLAUDIUS, King of Denmark, Hamlet’s uncle\n",
    "* The GHOST of the late king, Hamlet’s father\n",
    "* GERTRUDE, the Queen, Hamlet’s mother, now wife of Claudius\n",
    "* POLONIUS, Lord Chamberlain\n",
    "* LAERTES, Son to Polonius\n",
    "* OPHELIA, Daughter to Polonius\n",
    "* HORATIO, Friend to Hamlet\n",
    "* FORTINBRAS, Prince of Norway\n",
    "* VOLTEMAND, Courtier\n",
    "* CORNELIUS, Courtier\n",
    "* ROSENCRANTZ, Courtier\n",
    "* GUILDENSTERN, Courtier\n",
    "* MARCELLUS, Officer\n",
    "* BARNARDO, Officer\n",
    "* FRANCISCO, a Soldier\n",
    "* OSRIC, Courtier\n",
    "* REYNALDO, Servant to Polonius\n",
    "* Players\n",
    "* A Gentleman, Courtier\n",
    "* A Priest\n",
    "* Two Clowns, Grave-diggers\n",
    "* A Captain\n",
    "* English Ambassadors.\n",
    "* Lords, Ladies, Officers, Soldiers, Sailors, Messengers, and Attendants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc5073",
   "metadata": {},
   "source": [
    "# 4. Textual Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c3d3e",
   "metadata": {},
   "source": [
    "Then we count (old school style by reading the book) who is talking to whom, and we write that down in **G.add_edge()**. If we make a mistake and we accidentally write twice when a character talks to another one, it doesn´t matter. The networkx library will only take into acount one edge per pair of nodes. \n",
    "\n",
    "In theatre plays it can be a bit confussing to know who is talking to as some scenes (such as the last one in Hamlet) everybody is talking (or shouting!) at the same time and it is a total mess! It's ok if your edges are not 100% accurate: an approximation will be fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50b2d9",
   "metadata": {},
   "source": [
    "# 5. Checking the structure of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecabe15",
   "metadata": {},
   "source": [
    "Now let's have a look at the number of nodes that we have. Use the G.number_of_nodes() script and then transform G.nodes into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd2928",
   "metadata": {},
   "source": [
    "Let's do the same with the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e164827",
   "metadata": {},
   "source": [
    "And now let's check the weighted edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b967c",
   "metadata": {},
   "source": [
    "And now let's sort that list to see who talks the most!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf2fb3",
   "metadata": {},
   "source": [
    "Let's separate the edges based on their weights to visualize things better. This shows much clearly plot weight than our previous graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68323b4d",
   "metadata": {},
   "source": [
    "And finally let's get the position of the nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed634e",
   "metadata": {},
   "source": [
    "# 6. Network Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17cb11",
   "metadata": {},
   "source": [
    "This time, because we have a new element (weight) let's explore the network before we actually draw it. We do this because we are interested in tracking down the hub of the network (that is, the person with the biggest number of connections). We can create a network in which we assign those values (network degree) to the nodes, and we can quickly see the relationship between plot agency and hub size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd722f24",
   "metadata": {},
   "source": [
    "1. Calculating **Network Degree: who has more connections?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9a18d",
   "metadata": {},
   "source": [
    "Sort that variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909c53f",
   "metadata": {},
   "source": [
    "2. Calculating **Betweenes Centrality Scores**: who is the person that connects more nodes in the network? Sort your values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a5227",
   "metadata": {},
   "source": [
    "3. **Communities**: who forms different communities within this network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b35ea",
   "metadata": {},
   "source": [
    "# 7. Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253b4cd",
   "metadata": {},
   "source": [
    "And now let's have a look at our network! We can represent our weighted network a) by adding labels to the edges and showing the weight in there, or b) by showing the weight in different node sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac065b",
   "metadata": {},
   "source": [
    "# A. Edges weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df82313",
   "metadata": {},
   "source": [
    "# B. Nodes Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d511c",
   "metadata": {},
   "source": [
    "To draw a network by node weight, we need to know the network degree (**the hub**, and then, in order of importance, who has more weighted connections). Let's print again that value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a9ea9",
   "metadata": {},
   "source": [
    "Now let's change the colour of the hub of the network to red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d8b6f",
   "metadata": {},
   "source": [
    "# 8. Saving up our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5d9dd",
   "metadata": {},
   "source": [
    "Let's transform our network into a Pandas Dataframe. We can use nx.to_pandas_adjacency() to do this. It will return a Document Term Matrix where each node is assigned the number of times every character speaks (so, the weight), and 0 points if there is no interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832bbbe",
   "metadata": {},
   "source": [
    "#### Content from: Excercise 1. Modelling the Unweighted Social Network of Hamlet..ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa3aa3",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1d76c",
   "metadata": {},
   "source": [
    "Let's practice what we have just learnt! Let's follow this article (https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf) and built the social network of **Hamlet** by William Shakespeare. We will be using this version of the book: https://www.gutenberg.org/files/1524/1524-h/1524-h.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991178e",
   "metadata": {},
   "source": [
    "# 1. First we import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f21ca",
   "metadata": {},
   "source": [
    "# 2. The we create the G object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64134c9a",
   "metadata": {},
   "source": [
    "Just to let you know with this command we can clean our network (for example if we make a spelling mistake that contaminates our Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788d758",
   "metadata": {},
   "source": [
    "# 3. Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e564008",
   "metadata": {},
   "source": [
    "Now we transform every character into a node by writing each name inside **G.add_node()**. Only the main characters are included in here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521829",
   "metadata": {},
   "source": [
    "These are the play characters (you can find this information at the beginning of the book). Remember to change \"Claudius\" for \"King\" and \"Gertrude\" for \"Queen\" as that is how they will appear throughout the play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513efe6b",
   "metadata": {},
   "source": [
    "Dramatis Personæ\n",
    "\n",
    "* HAMLET, Prince of Denmark\n",
    "* CLAUDIUS, King of Denmark, Hamlet’s uncle\n",
    "* The GHOST of the late king, Hamlet’s father\n",
    "* GERTRUDE, the Queen, Hamlet’s mother, now wife of Claudius\n",
    "* POLONIUS, Lord Chamberlain\n",
    "* LAERTES, Son to Polonius\n",
    "* OPHELIA, Daughter to Polonius\n",
    "* HORATIO, Friend to Hamlet\n",
    "* FORTINBRAS, Prince of Norway\n",
    "* VOLTEMAND, Courtier\n",
    "* CORNELIUS, Courtier\n",
    "* ROSENCRANTZ, Courtier\n",
    "* GUILDENSTERN, Courtier\n",
    "* MARCELLUS, Officer\n",
    "* BARNARDO, Officer\n",
    "* FRANCISCO, a Soldier\n",
    "* OSRIC, Courtier\n",
    "* REYNALDO, Servant to Polonius\n",
    "* Players\n",
    "* A Gentleman, Courtier\n",
    "* A Priest\n",
    "* Two Clowns, Grave-diggers\n",
    "* A Captain\n",
    "* English Ambassadors.\n",
    "* Lords, Ladies, Officers, Soldiers, Sailors, Messengers, and Attendants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712237f",
   "metadata": {},
   "source": [
    "# 4. Textual Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf47d6",
   "metadata": {},
   "source": [
    "Then we count (old school style by reading the book) who is talking to whom, and we write that down in **G.add_edge()**. If we make a mistake and we accidentally write twice when a character talks to another one, it doesn´t matter. The networkx library will only take into acount one edge per pair of nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01961d",
   "metadata": {},
   "source": [
    "# 5. Checking the structure of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f766223",
   "metadata": {},
   "source": [
    "Now let's have a look at the number of nodes that we have. Use the G.number_of_nodes() script and then transform G.nodes into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bab530",
   "metadata": {},
   "source": [
    "Let's do the same with the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a41105",
   "metadata": {},
   "source": [
    "# 6. Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd2fd2",
   "metadata": {},
   "source": [
    "And now let's have a look at our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9be6b",
   "metadata": {},
   "source": [
    "# 7. Network metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81baa55",
   "metadata": {},
   "source": [
    "I looks like the tree characters that appear in the center of the network are Kat, Patrick and Joey. Let's try to discover who is the **hub**: the node of the network with the higher number of connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62709a26",
   "metadata": {},
   "source": [
    "1. Calculating **Network Degree**: who has more connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d69ccb",
   "metadata": {},
   "source": [
    "2. Calculating **Betweenes Centrality Scores**: who is the person that connects more nodes in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1bb4e",
   "metadata": {},
   "source": [
    "3. **Communities**: who forms different communities within this network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a183f",
   "metadata": {},
   "source": [
    "And then we can check whether there are some narrative sub-groups that tend to interact more with each other, and we do indeed observe four different communities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc620ee",
   "metadata": {},
   "source": [
    "# 8. Saving up our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8ad6b",
   "metadata": {},
   "source": [
    "Let's transform our network into a Pandas Dataframe. We can use nx.to_pandas_adjacency() to do this. It will return a Document Term Matrix where each node is assigned 1 point if there is an interaction between two characters, and 0 points if there is no interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac70d66",
   "metadata": {},
   "source": [
    "#### Content from: Ego Centric Networks. Exercise.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635e1ae",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889f4e1",
   "metadata": {},
   "source": [
    "# Ego-Centric Networks Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851afb1",
   "metadata": {},
   "source": [
    "Let's practice Ego-Centric Networks using Hamlet! Let's zoom in into the Ego-Centric Network of Hamlet himself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28b389",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf6d72",
   "metadata": {},
   "source": [
    "And let's not forget to creat G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a33e0",
   "metadata": {},
   "source": [
    "Reminder: how to clear our network if we need to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3ed88",
   "metadata": {},
   "source": [
    "# 2. Hamlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8d67e",
   "metadata": {},
   "source": [
    "Book in hand let's count how many times and with whom Hamlet talks to (we can re-use our previous Exercise 2 weighted social network script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23d21a",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9cc6f",
   "metadata": {},
   "source": [
    "# Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e91dc",
   "metadata": {},
   "source": [
    "# 3. We check the structure of our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb617b",
   "metadata": {},
   "source": [
    "Let's check the number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05894758",
   "metadata": {},
   "source": [
    "And now let's check the number of edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e38fd5",
   "metadata": {},
   "source": [
    "Let's check the weight of the edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5863c3",
   "metadata": {},
   "source": [
    "# 4. Network Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f703e2e",
   "metadata": {},
   "source": [
    "1. Calculating **Network Degree** (the hub of the network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2f627",
   "metadata": {},
   "source": [
    "The hub of the network is **Hamlet** because this is an Ego-Centric Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca49d8",
   "metadata": {},
   "source": [
    "# 5. Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395010a",
   "metadata": {},
   "source": [
    "First of all let's define the position of the nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87ebaa",
   "metadata": {},
   "source": [
    "Let's now use Network Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9575ae4",
   "metadata": {},
   "source": [
    "Voilá! There we have our network. We can now proceed to do some cool visualizations. For example, we can use colours to bring attention to the Hub in the network (Hamlet), indicate the second and their weighted degree nodes (Horatio and Pollonius), and change the colour of the other nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e18ae9",
   "metadata": {},
   "source": [
    "# And now you are a total expert on Networks ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace85af",
   "metadata": {},
   "source": [
    "#### Content from: Ego Centric Networks. .ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5d566",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d061f9",
   "metadata": {},
   "source": [
    "# Ego-Centric Networks vs Socio-Centric Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6124afa",
   "metadata": {},
   "source": [
    "In our previous data analysis (weighted vs unweighted networks), we have been building a type of network called socio-centric network, where we seek to analyze the social structure of a given community (characters in movies/novels/theatre plays). However, there is another type of network called ego-centric network, that uses individuals as their object of study. If you would like to know more about both, this is an excellent article: https://bebr.ufl.edu/sites/default/files/SNA_Encyclopedia_Entry_0.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a143e0",
   "metadata": {},
   "source": [
    "Let's use **Around the World in 80 days** and let's model one Ego-Centric Networks: Phileas Fogg. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e26fc",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e5a95",
   "metadata": {},
   "source": [
    "And let's not forget to creat G!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e6a62",
   "metadata": {},
   "source": [
    "# 2. Phileas Fogg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ab810",
   "metadata": {},
   "source": [
    "Book in hand let's count how many times and with whom Phileas Foogs talks to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482e6dd",
   "metadata": {},
   "source": [
    "#### Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f962f",
   "metadata": {},
   "source": [
    "# Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc25bd9",
   "metadata": {},
   "source": [
    "# 3. We check the structure of our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413eaae",
   "metadata": {},
   "source": [
    "Let's check the number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca8bd9",
   "metadata": {},
   "source": [
    "And now let's check the number of edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ab045",
   "metadata": {},
   "source": [
    "Let's check the weight of the edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392c42e",
   "metadata": {},
   "source": [
    "# 4. Network Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba2136",
   "metadata": {},
   "source": [
    "1. Calculating **Network Degree** (the hub of the network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3597cd",
   "metadata": {},
   "source": [
    "The hub of the network is **Phileas Fogg** because this is an Ego-Centric Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e5b79",
   "metadata": {},
   "source": [
    "# 5. Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87a375",
   "metadata": {},
   "source": [
    "First of all let's define the position of the nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839aecf",
   "metadata": {},
   "source": [
    "Let's now use Network Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0048b",
   "metadata": {},
   "source": [
    "Voilá! There we have our network. We can now proceed to do some cool visualizations. For example, we can use colours to bring attention to the Hub in the network (Phileas Fogg), indicate the second and their weighted degree nodes (Jean Passepartout and Auda), and change the colour of the other nodes in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914af4e2",
   "metadata": {},
   "source": [
    "# Exercise Ego Centric Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9178c03e",
   "metadata": {},
   "source": [
    "#### Content from: 2. Weighted Sociocentric Networks..ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2096a8a",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9def0",
   "metadata": {},
   "source": [
    "**References**: the majority of the scripts in this notebook come from adapting the previous one and asking questions to the language model **Perplexity AI** (for example: https://www.perplexity.ai/search/1fc28433-34c7-4240-a394-459ecb1475e1?s=u), which is a great way to teach-yourself how to write code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a0ba6",
   "metadata": {},
   "source": [
    "# Modeling a Weighted Network of Around the World in Eighty Days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91465a8",
   "metadata": {},
   "source": [
    "Now that we have calculated the social network (who knows whom) **Around the World in Eighty Days**, let's go one step further and let's calculate **how many times each character talks to each other**. So, book in hand, this time let's count not only when one character talks to each other once, but also how may times they do it. We will add one extra value in the edge part called \"weight\", where we will indicate with a number 1 every time there is a textual interaction.\n",
    "\n",
    "This is called a **weighted graph** (if you are interested about reading more, this is a good place to start: https://www.geeksforgeeks.org/applications-advantages-and-disadvantages-of-weighted-graph/). It adds extra information to unweighted graphs (like the one that we just built in the previous notebook). Let's see how these results either complement or contradict our previous ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73129d9f",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117f380",
   "metadata": {},
   "source": [
    "Then we create G (that we will use during all this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0638af",
   "metadata": {},
   "source": [
    "Remember that with this command we can clear our network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615032b",
   "metadata": {},
   "source": [
    "# 2. Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd8b2e",
   "metadata": {},
   "source": [
    "Now we transform every character into a node by writing each name inside **G.add_node()**, as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2eb01",
   "metadata": {},
   "source": [
    "# 2. Textual Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ef218",
   "metadata": {},
   "source": [
    "Now we add one new variable, \"weight = X\", where we include the number of times there is a dialogue between two characters. So: old school style, we grab the script, and with lots of patience, we just count dialogues (specifically: we count number of lines in each dialogue. For example: \n",
    "\n",
    "* **Phileas Fogg**: Good morning Passepartout!\n",
    "* **Passepartout**: Good morning sir!\n",
    "\n",
    "That would be represented as: \n",
    "\n",
    "**G.add_edge(\"Phileas Fogg\", \"Jean Passepartout\", weight = 2)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060226ef",
   "metadata": {},
   "source": [
    "# 3. Checking the structure of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccd6ed",
   "metadata": {},
   "source": [
    "Now let's have a look at the number of nodes that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c484a",
   "metadata": {},
   "source": [
    "Let's do the same with the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73494fac",
   "metadata": {},
   "source": [
    "And now let's check our weighted edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035d5ff",
   "metadata": {},
   "source": [
    "Let's sort that list to see which pair talks the most!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016240e8",
   "metadata": {},
   "source": [
    "Unsurprisingly, it is Jean Passepartout and Fix followed by Phileas Fogg and Passepartoutt. Let's separate the edges based on their weights to visualize things better. This shows much clearly plot weight than our previous graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c179866",
   "metadata": {},
   "source": [
    "And finally, let's define the position of the nodes in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59b42c",
   "metadata": {},
   "source": [
    "# 4. Network metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c82a1",
   "metadata": {},
   "source": [
    "This time, because we have a new element (**weight**) let's explore the network before we actually draw it. We do this because we are interested in tracking down **the hub of the network** (that is, the person with the biggest number of connections). We can create a network in which we assign those values (network degree) to the nodes, and we can quickly see the relationship between plot agency and hub size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b211f",
   "metadata": {},
   "source": [
    "1. Calculating **Network Degree**: who has more connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a35e53",
   "metadata": {},
   "source": [
    "So: **the network hub is Phileas Fogg!** He is the character that has the biggest number of connections, followed by Fix and Passepartout. These three characters are the ones that have by far the highest amount of text weight. Weighted Networks then add extra value to unweighted ones: now we can analyze **plot character agency**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba2330e",
   "metadata": {},
   "source": [
    "2. Calculating **Betweenes Centrality Scores**: who is the person that connects more nodes in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078716f9",
   "metadata": {},
   "source": [
    "So now that goes to Passepartout!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91520516",
   "metadata": {},
   "source": [
    "3. **Communities**: who forms different communities within this network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252ef1f",
   "metadata": {},
   "source": [
    "And now we can see different (and more coherent) clusters. We observe one Jean Passepartout cluster, one Phileas Fogg one, one English gentlemen one, and then two \"small weight characters\" ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097c7eb",
   "metadata": {},
   "source": [
    "# 5. Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34ccc0",
   "metadata": {},
   "source": [
    "And now let's have a look at our network! We can represent our weighted network a) by adding labels to the edges and showing the weight in there, or b) by showing the weight in different node sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562c727",
   "metadata": {},
   "source": [
    "# A. Edges weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3142ff",
   "metadata": {},
   "source": [
    "# B. Nodes Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0e88",
   "metadata": {},
   "source": [
    "To draw a network by node weight, we need to know the network degree (**the hub**, and then, in order of importance, who has more weighted connections). Let's print again that value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22810e7",
   "metadata": {},
   "source": [
    "We can also add different colours to specific nodes. For example, if we would like our **Hub** (Phileas Fogg), to stand up from the rest, we can colour her differently by adding a node_color argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a553638",
   "metadata": {},
   "source": [
    "We can repeat that process as many times as we would like to, by just adding more colours. Let's add two more hubs: **Jean Passepartout** and **Fix**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a93fb",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4adc37b",
   "metadata": {},
   "source": [
    "This network provides much fine-grained information than our previous one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830629d",
   "metadata": {},
   "source": [
    "# 7. Saving up our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1755337",
   "metadata": {},
   "source": [
    "Let's transform our network into a Pandas Dataframe. We can use nx.to_pandas_adjacency() to do this. It will return a Document Term Matrix where each node is assigned the number of times every character speaks (so, the weight), and 0 points if there is no interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad76c3",
   "metadata": {},
   "source": [
    "#### Content from: 1. Unweighted Sociocentric Networks. Modelling the Social Network of Around the World in 80 Days.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b0f33",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b18aa",
   "metadata": {},
   "source": [
    "**References**: the method of building a network using characters as nodes and textual interaction as edges is inspired by this article (https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf). While in there the text used is a play (and therefore it is easier to model networks), and we are using a novel instead, the idea is the same) Some of the scripts have been adapted from this tutorial (https://melaniewalsh.github.io/Intro-Cultural-Analytics/06-Network-Analysis/02-Making-Network-Viz-with-Bokeh.html), and also from this other one (https://networkx.org/documentation/stable/tutorial.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2dd0f",
   "metadata": {},
   "source": [
    "# Modelling the Social Network of **Around the World in 80 Days**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c83acf",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use **Network Analysis** to model the social network of **Around the World in Eighty Days**. We are going to count **\"who knows whom\"**. We are going to transform each character into a **nod**. After that, we are going to count every time that a character talks to another character, and we are going to call that **edge**. This method, while not perfect (it only measures textual interactions once, and therefore, we don´t know what is being said, or how much each character speaks), can be useful nevertheless as a first approximation of empirically measuring changing narrative weighths by identifying **hubs** (which essentially means nodes with lots of connections: https://en.wikipedia.org/wiki/Hub_(network_science)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beabc7b",
   "metadata": {},
   "source": [
    "# 1. First we import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d49cd",
   "metadata": {},
   "source": [
    "# 2. The we create the G object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c4f0d",
   "metadata": {},
   "source": [
    "Just to let you know with this command we can clean our network (for example if we make a spelling mistake that contaminates our Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9389554",
   "metadata": {},
   "source": [
    "# 3. Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0a882",
   "metadata": {},
   "source": [
    "Now we transform every character into a node by writing each name inside **G.add_node()**. Only the main characters are included in here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cd9db",
   "metadata": {},
   "source": [
    "# 4. Textual Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83161b62",
   "metadata": {},
   "source": [
    "Then we count (old school style by reading the book) who is talking to whom, and we write that down in **G.add_edge()**. If we make a mistake and we accidentally write twice when a character talks to another one, it doesn´t matter. The networkx library will only take into acount one edge per pair of nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8d0d2",
   "metadata": {},
   "source": [
    "# 5. Checking the structure of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c022eb",
   "metadata": {},
   "source": [
    "Now let's have a look at the number of nodes that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062efdb",
   "metadata": {},
   "source": [
    "Let's do the same with the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11445945",
   "metadata": {},
   "source": [
    "# 6. Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d0b39",
   "metadata": {},
   "source": [
    "And now let's have a look at our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a010fa",
   "metadata": {},
   "source": [
    "# 7. Network metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1169d",
   "metadata": {},
   "source": [
    "I looks like the tree characters that appear in the center of the network are Kat, Patrick and Joey. Let's try to discover who is the **hub**: the node of the network with the higher number of connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e08e2",
   "metadata": {},
   "source": [
    "1. Calculating **Network Degree**: who has more connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916f94e",
   "metadata": {},
   "source": [
    "So: the network hub is Phileas Fogg! He is the character that has the biggest number of connections, followed by Jean Passepartout. This could be considered as a preliminary metric of **plot character agency**, showing that in terms of who knows whom, these two characters are the most popular ones in the story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f696d5",
   "metadata": {},
   "source": [
    "2. Calculating **Betweenes Centrality Scores**: who is the person that connects more nodes in the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477dbd1",
   "metadata": {},
   "source": [
    "This is another metric to determine who can put more people in touch within the network. That position goes to Phileas Fogg, and by far!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5a8aa",
   "metadata": {},
   "source": [
    "3. **Communities**: who forms different communities within this network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47667ff7",
   "metadata": {},
   "source": [
    "And then we can check whether there are some narrative sub-groups that tend to interact more with each other, and we do indeed observe four different communities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3b177",
   "metadata": {},
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcdab8",
   "metadata": {},
   "source": [
    "While our network does not give any information about the content of the interactions between characters, or about how much they talk to each other (amount of text), it is a preliminary approach that shows how the hub of the network is Phileas Fogg, followed by Jean Passepartout, and therefore, these two characters direct the narrative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e77d2",
   "metadata": {},
   "source": [
    "# 9. Saving up our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee24bd",
   "metadata": {},
   "source": [
    "Let's transform our network into a Pandas Dataframe. We can use nx.to_pandas_adjacency() to do this. It will return a Document Term Matrix where each node is assigned 1 point if there is an interaction between two characters, and 0 points if there is no interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f69ac9",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d03d6",
   "metadata": {},
   "source": [
    "#### Content from: Harry Potter around the World.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03ad3e",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b858e",
   "metadata": {},
   "source": [
    "# Mapping the world of Harry Potter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64aef1",
   "metadata": {},
   "source": [
    "**Script source:** several queries to Perplexity AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e0fa7",
   "metadata": {},
   "source": [
    "Harry Potter is one of the most translated (and popular) books around the world and it is available in 85 languages! (https://en.wikipedia.org/wiki/List_of_Harry_Potter_translations)\n",
    "\n",
    "Let's write a python script to do a geo-spatial analysis visualization of things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d551e",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c79a5",
   "metadata": {},
   "source": [
    "# 2. We create a variable with the capitals of the countries where Harry Potter has been translated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1591861",
   "metadata": {},
   "source": [
    "In the real world you would need to do this step yourself! How would you do this using Python?\n",
    "\n",
    "And: every time there is more than one language in a country (i.e. South Africa: English and Afrikaans) I have used two cities in that country (i.e Pretoria and Cape Town) to show linguistic diversity!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c48b2",
   "metadata": {},
   "source": [
    "# 3. We create a list with the Lattitude and Longitude of those cities using Geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea6f44",
   "metadata": {},
   "source": [
    "Let's first practice getting the lat and lon of 3 English speaking main cities: London, Dublin, and New York City. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b8e37",
   "metadata": {},
   "source": [
    "Now let's do that for all the cities in our list!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c13c5f",
   "metadata": {},
   "source": [
    "# 4. Pandas Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99f43f",
   "metadata": {},
   "source": [
    "Now let's create a Pandas Dataframe that contains our cities and their lat and lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274bb35",
   "metadata": {},
   "source": [
    "First let's create a column with the names of the cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33df359",
   "metadata": {},
   "source": [
    "Now create two variables: one for latituted and one for longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531ac90",
   "metadata": {},
   "source": [
    "And now let's add those columns to our data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb33c9",
   "metadata": {},
   "source": [
    "# 5. And now let's visualize things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0fd19",
   "metadata": {},
   "source": [
    "Change the colour for red (for Gryffindor!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e91724",
   "metadata": {},
   "source": [
    "And now change the colour for green (for Slytherin!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17c1ec",
   "metadata": {},
   "source": [
    "#### Content from: Geospatial Analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2fa47",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14299f",
   "metadata": {},
   "source": [
    "# Using Geospatial Analysis to visually analyze Travel Literature!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd7e88",
   "metadata": {},
   "source": [
    "Geospatial Analysis can be a great tool to help us digg into the textual analysis of Literary Text. This can be particularly useful if we want to add extra layers of analysis to some genres such as **Travel Literature**. In this notebook we are going to exolore how to use the Python Library Plotly: https://plotly.com/python/getting-started/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586cfcf",
   "metadata": {},
   "source": [
    "**Sources:** the majority of the scripts in this notebook come from these sources from plotly: https://plotly.com/python/mapbox-layers/, https://plotly.com/python/scatter-plots-on-maps/, https://plotly.com/python/mapbox-layers/, https://plotly.com/python/reference/scattermapbox/#scattermapbox-marker-symbol. For more senior scripts about geo-spatial data science, this is an excellent course: https://github.com/suneman/socialdata2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ae6fb",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639bf8c",
   "metadata": {},
   "source": [
    "# 2. We manually inspect our city dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcaee43",
   "metadata": {},
   "source": [
    "All Digital Humanities projects involve some degree of close reading analysis. We need to inspect our \"GPE_aroundtheworld.txt\" file and decide which cities we are going to include in our selection! (you will see that there is a considereable ammount of noise even using Spacy, or that some place names are contemporary to the age of Jules Verne but have changed ever since)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a06e11",
   "metadata": {},
   "source": [
    "# 3. We create our GPS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fb646",
   "metadata": {},
   "source": [
    "To be able to map our cities, we need to extensively google the Latitude and Longitude of all of them, and manually annotate the results in several lists (as we will need to create a CSV dataframe to be able to plot things in maps with Plotly).\n",
    "\n",
    "Be aware that:\n",
    "\n",
    "**GPS Lat-Long signs: N+, S-, W-, E+.**\n",
    "\n",
    "For example:\n",
    "\n",
    "Rio de Janeiro: 22.9068° S, 43.1729° W (-22.9068, -43.1729)\n",
    "London: 51.5072° N, 0.1276° W (51.5072, -0.1276)\n",
    "Stockholm: 59.3293° N, 18.0686° E (59.3293, 18.0686)\n",
    "Sydney: 25.2744° S, 133.7751° E (-25.2744, -133.7751)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eead60",
   "metadata": {},
   "source": [
    "# Activity for you\n",
    "\n",
    "Please google \"Lat Long decimal\" and add the coordinates of **Denver, Bloomington (Indiana), Sacramento**. Add the lattitude, the longitude, and the country (at each corresponding list). Remember to remove the dots (that is just to indicate you where you should be writing things) and to write the closing braket of the list! Once you are finished run the scripts and you will automatically have a Pandas dataframe with all the information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4b679",
   "metadata": {},
   "source": [
    "# Geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb18f49",
   "metadata": {},
   "source": [
    "And now let's try another python library called GEOPY that will tell us the coordinates of our cities! If you are curious, you can read the documentation in here: https://geopy.readthedocs.io/en/stable/. For a faster tutorial you can have a look at https://pypi.org/project/geopy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87076d",
   "metadata": {},
   "source": [
    "Let's scale that to our full dataset of cities (so: if we have a file with all the GPE locations, we feed it into this script and it wil be super fast!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7effb03",
   "metadata": {},
   "source": [
    "When we get a none message it means that geopy does not know where is that city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93b9ce",
   "metadata": {},
   "source": [
    "# 5. And now we visualize things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eacd9f",
   "metadata": {},
   "source": [
    "Let's first try this map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977798c",
   "metadata": {},
   "source": [
    "##### A. Mapbox Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cbeb75",
   "metadata": {},
   "source": [
    "Mapbox maps are also called tile-based maps and they allow you to zoom in \"google maps\" style. For more information have a look at: https://plotly.com/python/mapbox-layers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57de35",
   "metadata": {},
   "source": [
    "#### Activity for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a087e6",
   "metadata": {},
   "source": [
    "Change the color_discrete_sequence = [] variable from \"fuschia\" to \"green\". You can try other colours!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3089b77",
   "metadata": {},
   "source": [
    "##### Activity for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a236f77",
   "metadata": {},
   "source": [
    "Move around your mouse on the top right corner of the map and click on the picture camera, where it says \"Download plot as PNG\". You will be able to download your map in your own laptop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4965f",
   "metadata": {},
   "source": [
    "##### B. Geo maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942c05d",
   "metadata": {},
   "source": [
    "Geo Maps only show the physical boundaries of countries. Have a look at: https://plotly.com/python/map-configuration/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2d8b7",
   "metadata": {},
   "source": [
    "Which one do you like the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb5b21",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b1992",
   "metadata": {},
   "source": [
    "#### Content from: Mapping Jules Verne. NER with Spacy.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d4311",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e02df",
   "metadata": {},
   "source": [
    "Now that we have done things at the chapter level, let's do it at the book level! Let's focus on mapping geographically the world of Jules verne by extracting GPE and LOC of **Around the World in 80 days**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8567c69",
   "metadata": {},
   "source": [
    "# 1. We import our libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b808de5",
   "metadata": {},
   "source": [
    "# 2. We get our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa35c37",
   "metadata": {},
   "source": [
    "This data has not been cleaned and pre-processed to avoid confusing the parser (only \\r\\n characters have been removed!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fba99",
   "metadata": {},
   "source": [
    "# 3. We import the English pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09088da9",
   "metadata": {},
   "source": [
    "# 4. We create the Spacy nlp object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a8469",
   "metadata": {},
   "source": [
    "# 5. We inspect the English model labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f6ddf",
   "metadata": {},
   "source": [
    "Let's remember the entities that we have in Spacy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f02734",
   "metadata": {},
   "source": [
    "# 6. We print the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b805d4",
   "metadata": {},
   "source": [
    "# 7. We create one list with GPE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4833f",
   "metadata": {},
   "source": [
    "While possibly LOC is a lable that contains interesting information, as this is a DH introductory course, let's just focus on GPE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1b6e8",
   "metadata": {},
   "source": [
    "Now let's drop the duplicates in there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ff391",
   "metadata": {},
   "source": [
    "Let's save our values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc74cb",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c4355",
   "metadata": {},
   "source": [
    "#### Content from: Information Extraction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7260e",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ff05a",
   "metadata": {},
   "source": [
    "# Information Extraction: NLTK and Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5efba6",
   "metadata": {},
   "source": [
    "Script Sources:\n",
    "\n",
    "* **NLTK**: Tsilimos, Maria. Python: Introduction to Natural Language Processing (NLP). IT Central, University of Zurich.\n",
    "* **Spacy**: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34f385",
   "metadata": {},
   "source": [
    "**Information Extraction (IE)** consists on transforming **Natural Language unstructured data** (written or spoken) into **structured data** ready to be used by machines. \n",
    "\n",
    "In this notebook we are going to learn two different IE methods: **Part of Speech Tagging (POS)** and **Name Entity Recognition (NER)**.\n",
    "\n",
    "There are many excellent Python libraries out there to write scripts that will allow us to do both things. In this notebook we will learn how to use **NLTK** and **Spacy** and understand the advantages and disadvantages of both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca6e9ef",
   "metadata": {},
   "source": [
    "# 1. Importing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849dc23",
   "metadata": {},
   "source": [
    "Let's begin by using the first chapter of **Around the World in Eighty Days** by Jules Verne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e075982",
   "metadata": {},
   "source": [
    "If you remember, in the previous chapter we did 4 steps of cleaning and pre-processing:\n",
    "\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Removing Punctuation\n",
    "* Removing Stopwords\n",
    "\n",
    "Now **we are not going to do any of those things**. We need to do **POS tagging**, and for that, it is necessary to keep punctuation and stopwords to avoid confusing the parser. \n",
    "\n",
    "The only thing that we are going to remove are the noisy characters \"\\r\\n\".\n",
    "\n",
    "For that, we are going to use this script: **re.sub(r\"\\r\\n\", \" \", data\")**. (in case you want to replicate it on your own dataset). \n",
    "\n",
    "For efficiency purposes a clean first chapter has been created for you with that process already incorporated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441485b",
   "metadata": {},
   "source": [
    "# 2. Understanding Information Extraction Architecture: NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41a98a",
   "metadata": {},
   "source": [
    "#### A. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b9a0e",
   "metadata": {},
   "source": [
    "#### B. We initialize the Information Extracture Pipeline:\n",
    "\n",
    "1. Sentence Segmentation\n",
    "2. Tokenization\n",
    "3. POS Tagging\n",
    "4. Chunking\n",
    "5. NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b847f0",
   "metadata": {},
   "source": [
    "##### 1. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f1a42",
   "metadata": {},
   "source": [
    "##### 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90504784",
   "metadata": {},
   "source": [
    "##### 3. POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a31158",
   "metadata": {},
   "source": [
    "##### 4. Chunking and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38910cbf",
   "metadata": {},
   "source": [
    "##### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8928c73",
   "metadata": {},
   "source": [
    "##### NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce2b4c",
   "metadata": {},
   "source": [
    "And now let's transform that into a list!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8008a",
   "metadata": {},
   "source": [
    "Source = https://nanonets.com/blog/named-entity-recognition-with-nltk-and-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6365734",
   "metadata": {},
   "source": [
    "That looks good so far! Let's now check **Geopolitical Entities (GPE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f67b75",
   "metadata": {},
   "source": [
    "That also looks quite good! However we observe some **issues**: is American or Londoner a person or a GPE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb620b5d",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236cfd3",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ba40f",
   "metadata": {},
   "source": [
    "And now let's try Spacy. Spacy does not follow the same architecture as NLTK: we don´t need to follow the 4 step pipeline (sentence segmentation, tokenization, POS tagging, NER chunking). All of that is implemented in their code! Have a look at: https://spacy.io/usage/linguistic-features#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd98acc",
   "metadata": {},
   "source": [
    "You may need to install the Spacy pipeline. If so, remove the #symbol in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4272a",
   "metadata": {},
   "source": [
    "Let's first have a look at the existing Entity Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65261d3",
   "metadata": {},
   "source": [
    "We have a winner!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3992e",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5beaeb",
   "metadata": {},
   "source": [
    "#### Content from: Exercises Information Extraction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6418076",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ea039",
   "metadata": {},
   "source": [
    "And now let's practice what we have just learnt but now with a multilingual text!\n",
    "\n",
    "Script Sources:\n",
    "\n",
    "* **NLTK**: Tsilimos, Maria. Python: Introduction to Natural Language Processing (NLP). IT Central, University of Zurich.\n",
    "* **Spacy**: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b56ba6",
   "metadata": {},
   "source": [
    "# Exercise 1: replicating the NLTK IE architecture with the first chapter of Twenty Thousand Leagues Under the Sea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea50ba",
   "metadata": {},
   "source": [
    "##### A. We import our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93482de",
   "metadata": {},
   "source": [
    "The second chapter of **Around the World in 80 days** has been created for you (without being cleaned and pre-processed, yet without \\r\\n characters). Write some code to open it!\n",
    "\n",
    "(P.S. Again, if you want to replicate the code for your own exercises, run the following script: import re re.sub(r\"\\r\\n\", \" \", data\")). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2d74f",
   "metadata": {},
   "source": [
    "##### B. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ef187",
   "metadata": {},
   "source": [
    "##### C. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827075a",
   "metadata": {},
   "source": [
    "##### D. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f43ff0",
   "metadata": {},
   "source": [
    "##### E. POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00b499",
   "metadata": {},
   "source": [
    "##### F. Chunking and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e028c91",
   "metadata": {},
   "source": [
    "##### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b268a",
   "metadata": {},
   "source": [
    "Try extracting a sentence that you like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ab8f1",
   "metadata": {},
   "source": [
    "Now create a tree out of that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047833d",
   "metadata": {},
   "source": [
    "##### G. Transforming that into a list and creating three different lists: 1. Person, 2. Organization, 3. GPE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed95e6d",
   "metadata": {},
   "source": [
    "1. Creating a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6b14a",
   "metadata": {},
   "source": [
    "2. Creating a person list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059970be",
   "metadata": {},
   "source": [
    "3. Creating a GPE list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce046c",
   "metadata": {},
   "source": [
    "4. Creating an organization list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94aad8",
   "metadata": {},
   "source": [
    "# Exercise 2: Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c750bf",
   "metadata": {},
   "source": [
    "Now let's repeat the exercise with Spacy to compare the performance of both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0155d6",
   "metadata": {},
   "source": [
    "##### A. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397444d3",
   "metadata": {},
   "source": [
    "##### B. We download the French SPACY pipeline and we inspect the entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f57df6",
   "metadata": {},
   "source": [
    "You may need to do this (remove the #symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e6c0e",
   "metadata": {},
   "source": [
    "##### C. We initialize the NLP object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2465fa",
   "metadata": {},
   "source": [
    "##### D. We create a list with the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84754551",
   "metadata": {},
   "source": [
    "##### E. We create three lists: one with person (PERSON), one with Geopolitical Entities (GPE), one with Organization (ORG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dcf6e",
   "metadata": {},
   "source": [
    "So: once again we see that Spacy really outperforms NLTK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1107c",
   "metadata": {},
   "source": [
    "#### Content from: PDF text extraction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e2438",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db7d7a",
   "metadata": {},
   "source": [
    "Script source: many queries to Perplexity AI (https://www.perplexity.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21e959",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c4461",
   "metadata": {},
   "source": [
    "We install the library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8083ed7",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd5963",
   "metadata": {},
   "source": [
    "And then we import it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571587d9",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77693313",
   "metadata": {},
   "source": [
    "We also import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec1ccc2",
   "metadata": {},
   "source": [
    "And the Operative System Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822f13b",
   "metadata": {},
   "source": [
    "# 2. We extract the text from the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218927ec",
   "metadata": {},
   "source": [
    "Let's select the PDF 2412.18779 that we have in our directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88463a",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "#Create empty list\n",
    "text = []\n",
    "\n",
    "# Open the PDF file\n",
    "with open('2412.18779.pdf', 'rb') as file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Get the number of pages in the PDF\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Initialize an empty string to store the extracted text\n",
    "    extracted_text = ''\n",
    "\n",
    "    # Loop through each page and extract the text\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        extracted_text += page.extract_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4180d3",
   "metadata": {},
   "source": [
    "# 3. We do that we with all our files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89fada",
   "metadata": {},
   "source": [
    "I have done a new query at the Arxiv notebook (check the notebook in this same folder!) using the term **Facebook**. Let's locate the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86da8d8",
   "metadata": {},
   "source": [
    "Now let's open it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f6c2d",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "# Set the directory path where the PDF files are located\n",
    "pdf_dir = 'C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 2\\\\PDF extraction\\\\arxiv_pdfs_facebook'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = os.listdir(pdf_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0ec30",
   "metadata": {},
   "source": [
    "There we have our files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1a547",
   "metadata": {},
   "source": [
    "Now let's extract all the text inside them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6cef31",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e63825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "# Loop through each PDF file\n",
    "extracted_texts = []\n",
    "\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(pdf_dir, file)  # Specify the directory where the files are located\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Get the number of pages in the PDF\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        # Initialize an empty string to store the extracted text\n",
    "        extracted_text = ''\n",
    "\n",
    "        # Loop through each page and extract the text\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            extracted_text += page.extract_text()\n",
    "    \n",
    "    # Add the extracted text to the list\n",
    "    extracted_texts.append(['Doc ' + file, extracted_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32dde1",
   "metadata": {},
   "source": [
    "# 4. We save that into our computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc466bf",
   "metadata": {},
   "source": [
    "First we extract the headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d0cf4",
   "metadata": {},
   "source": [
    "And now we extract the text of the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df10ce9",
   "metadata": {},
   "source": [
    "Now we create a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f47993",
   "metadata": {},
   "source": [
    "And finally we save it into our laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc181b1b",
   "metadata": {},
   "source": [
    "# EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4095e",
   "metadata": {},
   "source": [
    "Now let's create a new corpus of PDFs from ArXiv to use in our future data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c9a2c",
   "metadata": {},
   "source": [
    "* In this same folder you will find a notebook containing code to do an ArXiv query. So far we have used the key terms \"twitter\" and \"facebook\". Try doing a new one using a key term that is interesting to you. If you would like to use two words (Mark Zuckerberg, Climate Change, Donald Trump...) use this syntax: '\"climate change\"' (quotations inside quotations).\n",
    "\n",
    "* Now that you have your data, **duplicate this notebook** to have an extra copy of your code. Call the new version \"PDF text extraction EXERCISE\".\n",
    "\n",
    "* Once you have dubplicated your notebook and acquired your data, then first extract the text of one PDF (just like we do in here). Remember to change the name to the new file!\n",
    "\n",
    "* Then repeat the process with the whole folder. Remember to: \n",
    "                * Change the name of the folder in path_dir\n",
    "                * Change the name of the csv data folder containing your data to not overwrite your previous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41088c5d",
   "metadata": {},
   "source": [
    "#### Content from: Exercise Getting Data from Arxiv-2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051be8e0",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83251c1",
   "metadata": {},
   "source": [
    "# Accesing *ArXiv*\n",
    "\n",
    "## Elena Fernández Fernández"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f516da",
   "metadata": {},
   "source": [
    "Let's use **again** the ArXiv API to do a new query: https://pypi.org/project/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e5b58",
   "metadata": {},
   "source": [
    "# ArXiv API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c0abc",
   "metadata": {},
   "source": [
    "First let's import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157b88b",
   "metadata": {},
   "source": [
    "Let's build a query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29d674",
   "metadata": {},
   "source": [
    "And let's extract the 50 most recent Twitter ArXiv articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55f666",
   "metadata": {},
   "source": [
    "Ok, so now let's first have a look at the titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51e577",
   "metadata": {},
   "source": [
    "And now, if we compare that with the actual ArXiv website, it looks correct: https://arxiv.org/search/cs?query=twitter&searchtype=all&abstracts=show&order=-announced_date_first&size=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1513f",
   "metadata": {},
   "source": [
    "Now let's begin saving the first pdf in that list in our laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec52a497",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "paper = next(arxiv.Client().results(arxiv.Search(id_list = [\"2412.18779\"])))\n",
    "# Download the PDF to the PWD with a default filename.\n",
    "paper.download_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55abda10",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0002912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "# Download the PDF to the PWD with a custom filename.\n",
    "paper.download_pdf(filename = \"2412.18779.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d56e5",
   "metadata": {},
   "source": [
    "And now we need to create a folder in our directory. Remember that we can also do that using bash commands here in Jupyter Notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc2410",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf40fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "# Download the PDF to a specified directory with a custom filename.\n",
    "paper.download_pdf(dirpath = \"C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 2\\\\PDF extraction\\\\files_arxiv\",\n",
    "                    filename = \"2412.18779.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c44f6b",
   "metadata": {},
   "source": [
    "So far I have been using just the code provided by the Arxiv API to do all those things (https://pypi.org/project/arxiv/). Now let's go to the next level and let's extract a bunch of articles. Looking at the API it looks like we need to extract the IDs of the papers. Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4bba3f",
   "metadata": {},
   "source": [
    "What we need is just the ID number of the paper. Let's select that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c88f1d",
   "metadata": {},
   "source": [
    "And now let's loop around that to get all the PDFs into our laptop. First let's create a new folder called \"arxiv_pdfs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320e891",
   "metadata": {},
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f1632",
   "metadata": {},
   "source": [
    "REMEMBER to create a new folder for the new query that you are going to do to create your own PDF database. Remember to change it in the filename variable too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25db7c",
   "metadata": {},
   "source": [
    "And now let's get all the pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e713c",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "for id in ids_2:\n",
    "    # Search for the article with the given ID\n",
    "    search = arxiv.Search(id_list=[id])\n",
    "    paper = next(client.results(search))\n",
    "\n",
    "    # Download the PDF to the current working directory with a default filename\n",
    "    filename = os.path.join(\"arxiv_pdfs_facebook\", urllib.parse.quote(id))\n",
    "    paper.download_pdf(filename=f\"{filename}.pdf\")\n",
    "    \n",
    "    time.sleep(3)  # 3 seconds (this is the indication of the ArXiv API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e25cd",
   "metadata": {},
   "source": [
    "And voila! According to the arxiv API (https://pypi.org/project/arxiv/1.4.8/) the daily limit is 300.000 results: that is a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903cfa23",
   "metadata": {},
   "source": [
    "#### Content from: 5. Body.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d0df9",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179e51e",
   "metadata": {},
   "source": [
    "Now that we have our final dataframe, we still need to do some cleaning and preprocessing of our articles text. Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b009faa",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989370b6",
   "metadata": {},
   "source": [
    "# 2. We get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317ac6a",
   "metadata": {},
   "source": [
    "# 3. We select the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caab20",
   "metadata": {},
   "source": [
    "Checking if there are some float numbers (nan) that stand for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217db77",
   "metadata": {},
   "source": [
    "This is happening because we selected more metadata than proper articles (due to the 5000 download limit restrictions for full articles). So, there are some missing articles in there. Let's get rid of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bbe5d9",
   "metadata": {},
   "source": [
    "We have a clean dataframe! Let's go back to the body part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a49faa",
   "metadata": {},
   "source": [
    "# 4. We clean and pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee832d",
   "metadata": {},
   "source": [
    "Time to do some cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb48904",
   "metadata": {},
   "source": [
    "We can see that there is a rebel \\' character that has survived our cleaning function. Let's get rid of that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d064214",
   "metadata": {},
   "source": [
    "Now let's change our column in the csv dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fdbea",
   "metadata": {},
   "source": [
    "# 5. Saving our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2883d8",
   "metadata": {},
   "source": [
    "And now we are reading to save our super clean dataframe for future Text Data Mining analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa1326",
   "metadata": {},
   "source": [
    "#### Content from: 4. Merging Dataframes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155139a",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520b094",
   "metadata": {},
   "source": [
    "As we will see in the next Jupyter Notebook (5. Body) to be able to clean and pre-process the body we need to drop some missing rows of our dataframe that have some missing data. To simplify that process, let's now merge both dataframes before we proceed to cleaning the body of the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731151f4",
   "metadata": {},
   "source": [
    "# 1. We import our libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2b937",
   "metadata": {},
   "source": [
    "# 2. We get our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5a0a2",
   "metadata": {},
   "source": [
    "First we get the metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448f8ef",
   "metadata": {},
   "source": [
    "Now let's change the name of the column \"Gale Document Number\" to ID to be able to merge dataframes in just a second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ee278",
   "metadata": {},
   "source": [
    "And now we get the titles and the unclean body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6bb04",
   "metadata": {},
   "source": [
    "# 3. Let's merge dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8e7ae",
   "metadata": {},
   "source": [
    "Now let's merge both dataframes using the ID column on both of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c1669",
   "metadata": {},
   "source": [
    "# 4. Cleaning new Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f57e1",
   "metadata": {},
   "source": [
    "If we want to make sure that the merge was done correctly, we can check the \"Document Title\" column from the metadata column with the \"Title Column\" from the articles dataframe. That being said: let's clean this dataframe a little bit and get rid of the columns Publisher, Subject, and Language. Let's keep the Title one (and we can drop it later on if that may be useful for us)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be70f4",
   "metadata": {},
   "source": [
    "# 5. Saving our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d75519",
   "metadata": {},
   "source": [
    "And now let's save our data into a csv dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c189a",
   "metadata": {},
   "source": [
    "#### Content from: 3. Headers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de41f22",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554cab7c",
   "metadata": {},
   "source": [
    "Now let's begin by organizing (AKA cleaning and pre-processing) the titles (headers) of our articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e38b7c",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603edab",
   "metadata": {},
   "source": [
    "# 2. We get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70296fac",
   "metadata": {},
   "source": [
    "# 3. We split the title to get the CS indentifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337071f7",
   "metadata": {},
   "source": [
    "The way in which we are going to be able to match data (titles and articles) with metadata is by doing a match between the CS identifier in both dataframes. So: we need to extract that from the titles of the articles in here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c33e6e",
   "metadata": {},
   "source": [
    "First we split things by \"CS\" (an alternative way would be to do this using regex but it's much more complicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45ffc2",
   "metadata": {},
   "source": [
    "And now we need to add CS again to make sure that we can later on concatenate it with the Metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fdb59",
   "metadata": {},
   "source": [
    "And now we need to get rid of the final .txt to be able to later on match things with the metadata dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f368e4",
   "metadata": {},
   "source": [
    "# 4. And now we create a new CSV data frame with a new column: Article ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e036f975",
   "metadata": {},
   "source": [
    "First we break that list into two different ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65095864",
   "metadata": {},
   "source": [
    "And now we create the new csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c807f",
   "metadata": {},
   "source": [
    "And now we link that to the original dataframe with the proper text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c3d4c",
   "metadata": {},
   "source": [
    "So now we have our clean dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6f07e",
   "metadata": {},
   "source": [
    "# 5. We export everything into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1ebed",
   "metadata": {},
   "source": [
    "#### Content from: 2. Importing Text Data Gale.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e91624",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461c260",
   "metadata": {},
   "source": [
    "Now that we have our dataframe with the Metadata, let's find a way to use the text files that we can download from the Gale. First, let's import them into our computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9039b3",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e5b55",
   "metadata": {},
   "source": [
    "# 2. We set a Path to get the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89684fa9",
   "metadata": {},
   "source": [
    "To be able to access the files, we need to first find where they are located in our computer. So, we need to set a path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad464c78",
   "metadata": {},
   "source": [
    "# 3. We create the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2957e",
   "metadata": {},
   "source": [
    "Now that we have the files, we need to import them into our laptop and create a datafarame with titles in one column and the text of the article in another column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e39c40",
   "metadata": {},
   "source": [
    "# 4. We export the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66417f2",
   "metadata": {},
   "source": [
    "Success! We have our dataframe and we are ready to export it to a CSV file to start the process of cleaning and pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5458ff",
   "metadata": {},
   "source": [
    "#### Content from: 1. Gale Metadata.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933df51f",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3497b",
   "metadata": {},
   "source": [
    "Let's begin transforming the Gale Metadata Dataframe into something that we can use to later on merge with our text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f53615",
   "metadata": {},
   "source": [
    "# 1. We impor the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ed46a",
   "metadata": {},
   "source": [
    "# 2. We import our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc5d590",
   "metadata": {},
   "source": [
    "# 3. We modify the metadata column that we need to do the matching later on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fcfb4",
   "metadata": {},
   "source": [
    "To be able to do a matching between the text dataframe and this column, we need to remove \"GALE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfcff5",
   "metadata": {},
   "source": [
    "Now let's substitute the original column with that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24bcfb",
   "metadata": {},
   "source": [
    "# 4. We export that to a CSV dataframe that we can use later on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0084353",
   "metadata": {},
   "source": [
    "#### Content from: Exercises cleaning and pre-processing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f41ef",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310d894",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4b6e0",
   "metadata": {},
   "source": [
    "Let's practice what we have just learnt!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2d796",
   "metadata": {},
   "source": [
    "# Exercise 1. Multilingual cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ab58e",
   "metadata": {},
   "source": [
    "##### A. Import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683a904",
   "metadata": {},
   "source": [
    "##### B. Open the file that you created in the first exercise (**Vingt milles lieues sous les mers**). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ee796",
   "metadata": {},
   "source": [
    "##### C. Extract the text of the chapters into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1058189",
   "metadata": {},
   "source": [
    "##### D. Tokenize your text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d174cdad",
   "metadata": {},
   "source": [
    "##### E. Lowercase your text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94c4d1",
   "metadata": {},
   "source": [
    "##### F. Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113f0bd",
   "metadata": {},
   "source": [
    "##### G. Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56215c",
   "metadata": {},
   "source": [
    "##### F. Check out Stopwords in German (this is useful for those of you who want to use German text!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b6ab51",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69366227",
   "metadata": {},
   "source": [
    "##### A. Create a Pandas Dataframe with your clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f52c0",
   "metadata": {},
   "source": [
    "##### B. Save that into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022406e9",
   "metadata": {},
   "source": [
    "##### C. Transform your super_clean variable (a list) into a single string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d5331",
   "metadata": {},
   "source": [
    "##### D. Store that into a txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8eaf10",
   "metadata": {},
   "source": [
    "#### Content from: Cleaning and preprocessing data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fadcb",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ab3e3",
   "metadata": {},
   "source": [
    "# Step 2. Cleaning and Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d493c",
   "metadata": {},
   "source": [
    "Now that you have your data (Webscraping, APIs, PDFs, databases...) the next step in our Digital Humanities project is **cleaning and pre-processing**. In this notebook we are going to use the file that we just created in the previous notebook (*Around the World in Eighty Days*) and we are going to:\n",
    "\n",
    "* Tokenize\n",
    "* Lowercase\n",
    "* Remove Punctuation\n",
    "* Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150a23a",
   "metadata": {},
   "source": [
    "# 1. We import our libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed23d9",
   "metadata": {},
   "source": [
    "# 2. We get our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71bdee",
   "metadata": {},
   "source": [
    "# 3. We extract the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89efe7",
   "metadata": {},
   "source": [
    "# 4. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea3ad5",
   "metadata": {},
   "source": [
    "# 5. Lower casing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338716e0",
   "metadata": {},
   "source": [
    "To lower case our data, we need to right a double loop, as we are looping over each element (tokens) contained in each element of the list. So, first we loop over the list, and then we loop over each token in each list item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09ad16",
   "metadata": {},
   "source": [
    "# 6. Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91579b02",
   "metadata": {},
   "source": [
    "# 7. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4ae6a",
   "metadata": {},
   "source": [
    "Let's first have a look at stopwords in English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19277511",
   "metadata": {},
   "source": [
    "Great! We have 37 very clean chapters of **Around the World in 80 days**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f7170",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a579950",
   "metadata": {},
   "source": [
    "# 8. Using that data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b18ad3",
   "metadata": {},
   "source": [
    "Now that we have **very clean data**, we have two options:\n",
    "    \n",
    "    1. We use it chapter by chapter the way we have it (in case we may want to see how things evolve over the novel)\n",
    "    2. We transform it into a single string in case we may want to analyse the whole book at once.\n",
    "    \n",
    "Let's do both things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4f012",
   "metadata": {},
   "source": [
    "**8.1. Chapters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c669799",
   "metadata": {},
   "source": [
    "We can stor things into a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13e1fb",
   "metadata": {},
   "source": [
    "Or into a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294dc79",
   "metadata": {},
   "source": [
    "And now let's save that into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fced46",
   "metadata": {},
   "source": [
    "**8.2. Full text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38282f9",
   "metadata": {},
   "source": [
    "Let's now save that into a txt file format (which is widely used in Digital Humanities!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b7177",
   "metadata": {},
   "source": [
    "You should now have that file into your laptop! We will be using it during the following days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108e706",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6b459",
   "metadata": {},
   "source": [
    "#### Content from: 1. API The Guardian.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0281418",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e904a",
   "metadata": {},
   "source": [
    "**References**: the code in this notebook comes from this script (written by my baby-programmer self 5 years ago: https://github.com/effernan/New-York-Times-Archive-API-code), and this more senior script: https://github.com/rochelleterman/scrape-interwebz/blob/master/1_APIs/3_api_workbook.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbcf32",
   "metadata": {},
   "source": [
    "# The Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9c2c9",
   "metadata": {},
   "source": [
    "In this exercise we will be using the API of **The Guardian** (that does provide the full text of articles): https://open-platform.theguardian.com/access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ebec5",
   "metadata": {},
   "source": [
    "First, you need a key, that you can get in here: https://bonobo.capi.gutools.co.uk/register/developer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bce1a8",
   "metadata": {},
   "source": [
    "Let's explore **The Guardian** documentation website: https://open-platform.theguardian.com/documentation/\n",
    "\n",
    "There are **5 endpoints**: Content, Tags, Sections, Editions, Single item. The one that we need is **content**: https://open-platform.theguardian.com/documentation/search \n",
    "\n",
    "So, in there, we have all the different options that the API is providing. The base_url is: \"https://content.guardianapis.com/search?\"\n",
    "\n",
    "Everything else is very similar to The New York Times, but in here, there is one option that we can include in the parameters that will return the full text of the articles: \"\"show-fields\" : \"body\". How **super cool** is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc37205",
   "metadata": {},
   "source": [
    "##### 1. Let's build the API request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890019b5",
   "metadata": {},
   "source": [
    "Now the keys have changed! What we need to access is \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ec9bb",
   "metadata": {},
   "source": [
    "Now let's build the proper call modifying our previous script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c48228",
   "metadata": {},
   "source": [
    "Let's have a look at that first element in our list of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4450f98",
   "metadata": {},
   "source": [
    "So, what we want is: id, webPublicationDate, webTitle, webUrl, and the content of the article (that is in fields). Let's modify our function to get us that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ef8ab",
   "metadata": {},
   "source": [
    "And now let's store that. We have a super cool David Beckham dataset that we can use for our future data analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5207a9",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d07ae",
   "metadata": {},
   "source": [
    "And now: repeat the exercise but enter some term that you may be interested about (i.e. another Athlete, or some other group of news that you would like to see.) Remember to change the name of the csv file to not overwrite your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6acf4be",
   "metadata": {},
   "source": [
    "#### Content from: Exercise Getting Data from Arxiv.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c93d0f",
   "metadata": {},
   "source": [
    "##### Annotation:\n",
    "This section includes the content and logic from the original notebook. Annotations have been added to explain the purpose and function of each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a716d",
   "metadata": {},
   "source": [
    "# Accesing *ArXiv*\n",
    "\n",
    "## Elena Fernández Fernández"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35bbc4",
   "metadata": {},
   "source": [
    "Let's give a try to webscrape the ArXiv website (disclaimer: I previously emailed them and ask if it was ok and legal to do this and they said yes!).\n",
    "\n",
    "ArXiv is one of the most popular Computer Science article repositories out there and a great resource for Text Data Mining Research! Let's try to get the latest 50 articles about Twitter: https://arxiv.org/search/cs?query=twitter&searchtype=all&abstracts=show&order=-announced_date_first&size=50\n",
    "\n",
    "If you right click on your mouse, you will see that the source code of the website is very similar to La Gaceta de Madrid. So: let's try to re-use that script for this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fa53b",
   "metadata": {},
   "source": [
    "The first thing that you need to do is to import the necessary libraries for webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a8b91",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil #this one is for saving the PDFs from our computer.\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb5963",
   "metadata": {},
   "source": [
    "Then let's put the url into our laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a3c9d",
   "metadata": {},
   "source": [
    "And now let's access the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901878ed",
   "metadata": {},
   "source": [
    "The PDFs, which is what we are looking for, are contained in the category \"a\", so we need to filter our search to get all the information stored within \"a\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43aeef",
   "metadata": {},
   "source": [
    "Once we have all the \"a\" information, we need to define our search even more, as the PDFs links that we are looking for are stored within the \"href\" category inside of \"a\". We will store them in a list (pdfs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61321d37",
   "metadata": {},
   "source": [
    "Now we need to narrow our search even more. What we need are all the strings in which the PDFs are stored. Let's try to get them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b6af3",
   "metadata": {},
   "source": [
    "Something is not working. What is it? Let's ask Perplexity AI: https://www.perplexity.ai/search/Im-trying-to-U7GFFcNcTb6df18PJ_RpZg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea57814",
   "metadata": {},
   "source": [
    "So: the ArXiv web developers have built some sort of mechanisms that do not allow us to webscrape their website! But good news: we can use their API: https://pypi.org/project/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f6efd",
   "metadata": {},
   "source": [
    "# ArXiv API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0868da",
   "metadata": {},
   "source": [
    "First let's install the python library arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d697cb",
   "metadata": {},
   "source": [
    "And now let's import it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f48a14",
   "metadata": {},
   "source": [
    "Let's also import the time library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc333a",
   "metadata": {},
   "source": [
    "Let's build a query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eedc7c",
   "metadata": {},
   "source": [
    "And let's extract the 50 most recent Twitter ArXiv articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41218e",
   "metadata": {},
   "source": [
    "Ok, so now let's first have a look at the titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435f253",
   "metadata": {},
   "source": [
    "And now, if we compare that with the actual ArXiv website, it looks correct: https://arxiv.org/search/cs?query=twitter&searchtype=all&abstracts=show&order=-announced_date_first&size=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099db2fd",
   "metadata": {},
   "source": [
    "Now let's begin saving the first pdf in that list in our laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe14857",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcedde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"2406.12444\"])))\n",
    "# Download the PDF to the PWD with a default filename.\n",
    "paper.download_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e154a2",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "# Download the PDF to the PWD with a custom filename.\n",
    "paper.download_pdf(filename=\"2406.12444v1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a27dbe",
   "metadata": {},
   "source": [
    "And now we need to create a folder in our directory. Remember that we can also do that using bash commands here in Jupyter Notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ec174",
   "metadata": {},
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a3b0bf",
   "metadata": {},
   "source": [
    "REMEMBER to create a new folder for the new query that you are going when you will be doing the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136a1ef",
   "metadata": {},
   "source": [
    "And remember to change the name of the folder in the dirpath when you do the exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb2546",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "# Download the PDF to a specified directory with a custom filename.\n",
    "paper.download_pdf(dirpath = 'C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 1\\\\APIs 1. Arxiv\\\\files_arxiv',\n",
    "                    filename = \"2406.12444v1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d0992",
   "metadata": {},
   "source": [
    "So far I have been using just the code provided by the Arxiv API to do all those things (https://pypi.org/project/arxiv/). Now let's go to the next level and let's extract a bunch of articles. Looking at the API it looks like we need to extract the IDs of the papers. Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116598da",
   "metadata": {},
   "source": [
    "What we need is just the ID number of the paper. Let's select that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324c9e5",
   "metadata": {},
   "source": [
    "And now let's loop around that to get all the PDFs into our laptop. First let's create a new folder called \"arxiv_pdfs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d94cb4",
   "metadata": {},
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f31f4b",
   "metadata": {},
   "source": [
    "REMEMBER to create a new folder for the new query that you are going when you will be doing the exercise. Remember to change the name of the folder in filename down there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b185e41",
   "metadata": {},
   "source": [
    "And now let's get all the pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a909a2",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION (auf Deutsch):\n",
    "# Dieser Code führt die folgende Aufgabe aus:\n",
    "# (Fügen Sie hier eine spezifische Erklärung hinzu, basierend auf dem Code.)\n",
    "# This cell performs the following task:\n",
    "# (Add specific annotations here based on the logic in the cell)\n",
    "\n",
    "for id in ids_2:\n",
    "    # Search for the article with the given ID\n",
    "    search = arxiv.Search(id_list=[id])\n",
    "    paper = next(client.results(search))\n",
    "\n",
    "    # Download the PDF to the current working directory with a default filename\n",
    "    filename = os.path.join(\"arxiv_pdfs\", urllib.parse.quote(id))\n",
    "    result.download_pdf(filename=f\"{filename}.pdf\")\n",
    "    \n",
    "    time.sleep(3)  # 3 seconds (this is the indication of the ArXiv API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ca289",
   "metadata": {},
   "source": [
    "And voila! According to the arxiv API (https://pypi.org/project/arxiv/1.4.8/) the daily limit is 300.000 results: that is a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a99699",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b30f1",
   "metadata": {},
   "source": [
    "Now use this same notebook and do a new search using a different key term. \n",
    "* If you would like to use two words (Mark Zuckerberg, Climate Change, Donald Trump...) use this syntax: '\"climate change\"' (quotations inside quotations).\n",
    "* Remember to **change the name of the folders** when you will be creating new ones for the individual PDF and for the list of PDFs to not have both queries all mixed up in the same folder"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
