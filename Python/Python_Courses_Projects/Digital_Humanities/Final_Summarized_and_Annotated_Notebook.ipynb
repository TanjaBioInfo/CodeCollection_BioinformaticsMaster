{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4252500b",
   "metadata": {},
   "source": [
    "# Zusammenfassung: PDF- und HTML-Scraping\n",
    "Dieses Notebook kombiniert Inhalte aus den hochgeladenen Notebooks und zeigt anhand von ausgewählten Beispielen, wie man Daten aus PDFs extrahiert (PDF-Scraping) und HTML-Webseiten verarbeitet (HTML-Scraping). Alle Code- und Text-Chunks sind auf Deutsch annotiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50679d",
   "metadata": {},
   "source": [
    "### Beispiel: PDF-Scraping\n",
    "#### Erklärung (Deutsch):\n",
    "Dieser Abschnitt zeigt, wie {description.lower()} funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ab278",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcecd77",
   "metadata": {},
   "source": [
    "### Beispiel: API-Datenabfrage\n",
    "#### Erklärung (Deutsch):\n",
    "Dieser Abschnitt zeigt, wie {description.lower()} funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ef3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time  # to pause after each API call \n",
    "from __future__ import division\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # to see our CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b210fc0",
   "metadata": {},
   "source": [
    "### Beispiel: Datenbereinigung\n",
    "#### Erklärung (Deutsch):\n",
    "Dieser Abschnitt zeigt, wie {description.lower()} funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "\n",
    "super_clean = []\n",
    "for i in punctuation_free:\n",
    "    super_clean.append(\" \".join(c for c in i.split() if c not in stopwords.words(\"french\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba3ac5",
   "metadata": {},
   "source": [
    "### Inhalte aus: 1. Gale Metadata.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d36931",
   "metadata": {},
   "source": [
    "Let's begin transforming the Gale Metadata Dataframe into something that we can use to later on merge with our text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997bfd1",
   "metadata": {},
   "source": [
    "# 1. We impor the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b0cd7d",
   "metadata": {},
   "source": [
    "# 2. We import our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"Times 1980 January February_metadata.csv\", index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5649124",
   "metadata": {},
   "source": [
    "# 3. We modify the metadata column that we need to do the matching later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3256fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = metadata[\"Gale Document Number\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a316979",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042815d",
   "metadata": {},
   "source": [
    "To be able to do a matching between the text dataframe and this column, we need to remove \"GALE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = []\n",
    "\n",
    "for i in text:\n",
    "    new_text.append(i.replace('GALE|', ''))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b976803",
   "metadata": {},
   "source": [
    "Now let's substitute the original column with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58098a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"Gale Document Number\"] = new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe8551",
   "metadata": {},
   "source": [
    "# 4. We export that to a CSV dataframe that we can use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a62b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv(\"final_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2d6d6",
   "metadata": {},
   "source": [
    "### Inhalte aus: 2. Importing Text Data Gale.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d5263",
   "metadata": {},
   "source": [
    "Now that we have our dataframe with the Metadata, let's find a way to use the text files that we can download from the Gale. First, let's import them into our computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39be541",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c101c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25947f",
   "metadata": {},
   "source": [
    "# 2. We set a Path to get the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b274d",
   "metadata": {},
   "source": [
    "To be able to access the files, we need to first find where they are located in our computer. So, we need to set a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baec304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476deff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"yourpath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f628f8",
   "metadata": {},
   "source": [
    "# 3. We create the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c58910",
   "metadata": {},
   "source": [
    "Now that we have the files, we need to import them into our laptop and create a datafarame with titles in one column and the text of the article in another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.txt'):  # Ensure you're only processing text files\n",
    "        with open(os.path.join(directory_path, filename), 'r') as file:\n",
    "            titles.append(filename)  # Store the filename as title\n",
    "            contents.append(file.read())  # Read and store the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee086897",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81269ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453df140",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Content': contents\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d3ef1",
   "metadata": {},
   "source": [
    "# 4. We export the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8d947",
   "metadata": {},
   "source": [
    "Success! We have our dataframe and we are ready to export it to a CSV file to start the process of cleaning and pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ceec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('raw_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0b15c",
   "metadata": {},
   "source": [
    "### Inhalte aus: 3. Headers.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6bce2",
   "metadata": {},
   "source": [
    "Now let's begin by organizing (AKA cleaning and pre-processing) the titles (headers) of our articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad945e2d",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cd975",
   "metadata": {},
   "source": [
    "# 2. We get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5002ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"raw_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96dd0f",
   "metadata": {},
   "source": [
    "# 3. We split the title to get the CS indentifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c60b8e",
   "metadata": {},
   "source": [
    "The way in which we are going to be able to match data (titles and articles) with metadata is by doing a match between the CS identifier in both dataframes. So: we need to extract that from the titles of the articles in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a465247",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = data[\"Title\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bee1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea821a",
   "metadata": {},
   "source": [
    "First we split things by \"CS\" (an alternative way would be to do this using regex but it's much more complicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [s.split('CS') for s in title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84543f",
   "metadata": {},
   "source": [
    "And now we need to add CS again to make sure that we can later on concatenate it with the Metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_data = [[inner[0], 'CS' + inner[1]] for inner in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f0b358",
   "metadata": {},
   "source": [
    "And now we need to get rid of the final .txt to be able to later on match things with the metadata dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5035a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [[item[0], item[1].replace('.txt', '')] for item in modified_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c605f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d52cdee",
   "metadata": {},
   "source": [
    "# 4. And now we create a new CSV data frame with a new column: Article ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4f5a9",
   "metadata": {},
   "source": [
    "First we break that list into two different ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = [i[0] for i in cleaned_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe620ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fb804",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_articles = [i[1] for i in cleaned_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ecbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b6d4",
   "metadata": {},
   "source": [
    "And now we create the new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(title, columns = [\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"ID\"] = id_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a277e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c149453",
   "metadata": {},
   "source": [
    "And now we link that to the original dataframe with the proper text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f868f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[\"Content\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b90628",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"Article\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af9872",
   "metadata": {},
   "source": [
    "So now we have our clean dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12df62",
   "metadata": {},
   "source": [
    "# 5. We export everything into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"headers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd3825",
   "metadata": {},
   "source": [
    "### Inhalte aus: 4. Merging Dataframes.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66a99f",
   "metadata": {},
   "source": [
    "As we will see in the next Jupyter Notebook (5. Body) to be able to clean and pre-process the body we need to drop some missing rows of our dataframe that have some missing data. To simplify that process, let's now merge both dataframes before we proceed to cleaning the body of the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e77807",
   "metadata": {},
   "source": [
    "# 1. We import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519157e3",
   "metadata": {},
   "source": [
    "# 2. We get our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77815e27",
   "metadata": {},
   "source": [
    "First we get the metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"final_metadata.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240e3be",
   "metadata": {},
   "source": [
    "Now let's change the name of the column \"Gale Document Number\" to ID to be able to merge dataframes in just a second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.rename(columns = {\"Gale Document Number\" :\"ID\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73860e",
   "metadata": {},
   "source": [
    "And now we get the titles and the unclean body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"headers.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbb2dd",
   "metadata": {},
   "source": [
    "# 3. Let's merge dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb7b29",
   "metadata": {},
   "source": [
    "Now let's merge both dataframes using the ID column on both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(metadata, articles, on = 'ID', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40eb7e4",
   "metadata": {},
   "source": [
    "# 4. Cleaning new Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da60ce1",
   "metadata": {},
   "source": [
    "If we want to make sure that the merge was done correctly, we can check the \"Document Title\" column from the metadata column with the \"Title Column\" from the articles dataframe. That being said: let's clean this dataframe a little bit and get rid of the columns Publisher, Subject, and Language. Let's keep the Title one (and we can drop it later on if that may be useful for us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = merged_df.drop(['Publisher', 'Subject', 'Language'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a7b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc067f74",
   "metadata": {},
   "source": [
    "# 5. Saving our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4c825",
   "metadata": {},
   "source": [
    "And now let's save our data into a csv dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6298940",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2297a26",
   "metadata": {},
   "source": [
    "### Inhalte aus: 5. Body.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0513b",
   "metadata": {},
   "source": [
    "Now that we have our final dataframe, we still need to do some cleaning and preprocessing of our articles text. Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf9cdd",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592525c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f3881",
   "metadata": {},
   "source": [
    "# 2. We get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513898c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_data.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ca8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763adb3a",
   "metadata": {},
   "source": [
    "# 3. We select the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d18bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = data[\"Article\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c723fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24361de",
   "metadata": {},
   "outputs": [],
   "source": [
    "body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(body[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c886a4",
   "metadata": {},
   "source": [
    "Checking if there are some float numbers (nan) that stand for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fdc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in body:\n",
    "    if type(i) == float:\n",
    "        print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in enumerate(body):\n",
    "    if isinstance(value, float):\n",
    "        print(f\"Index: {index}, Value Type: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "body[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[228:229]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a8050",
   "metadata": {},
   "source": [
    "This is happening because we selected more metadata than proper articles (due to the 5000 download limit restrictions for full articles). So, there are some missing articles in there. Let's get rid of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ad094",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data.dropna(subset = [\"Article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a8aac",
   "metadata": {},
   "source": [
    "We have a clean dataframe! Let's go back to the body part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = data_clean[\"Article\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "body[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc7df1",
   "metadata": {},
   "source": [
    "# 4. We clean and pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db96bfc",
   "metadata": {},
   "source": [
    "Time to do some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = [i.replace(\"\\n\", \"\") for i in text]\n",
    "    final_text = [i.replace(\"\\\\\", \"\") for i in clean_text]\n",
    "    really_final_text = [i.replace(\"\\\\'\", \"\") for i in final_text]\n",
    "    return really_final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb39a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = clean_text(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1811a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097d4e7",
   "metadata": {},
   "source": [
    "We can see that there is a rebel \\' character that has survived our cleaning function. Let's get rid of that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a215aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_final(text):\n",
    "    final_clean_text = [i.replace(\"\\n\", \"\").replace(\"\\\\\", \"\").replace(\"'\", \"\") for i in text]\n",
    "    return final_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "really_final_text = clean_text_final(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0379c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "really_final_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04e42a",
   "metadata": {},
   "source": [
    "Now let's change our column in the csv dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c99b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.loc[:, \"Article\"] = really_final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92686fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e103ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[\"Article\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96d75d",
   "metadata": {},
   "source": [
    "# 5. Saving our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0addd",
   "metadata": {},
   "source": [
    "And now we are reading to save our super clean dataframe for future Text Data Mining analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.to_csv(\"final_TDM_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24ba4e",
   "metadata": {},
   "source": [
    "### Inhalte aus: Exercises Information Extraction.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e124766",
   "metadata": {},
   "source": [
    "And now let's practice what we have just learnt but now with a multilingual text!\n",
    "\n",
    "Script Sources:\n",
    "\n",
    "* **NLTK**: Tsilimos, Maria. Python: Introduction to Natural Language Processing (NLP). IT Central, University of Zurich.\n",
    "* **Spacy**: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1ec5a",
   "metadata": {},
   "source": [
    "# Exercise 1: replicating the NLTK IE architecture with the first chapter of Twenty Thousand Leagues Under the Sea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99766be",
   "metadata": {},
   "source": [
    "#### A. We import our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff2f96",
   "metadata": {},
   "source": [
    "The second chapter of **Around the World in 80 days** has been created for you (without being cleaned and pre-processed, yet without \\r\\n characters). Write some code to open it!\n",
    "\n",
    "(P.S. Again, if you want to replicate the code for your own exercises, run the following script: import re re.sub(r\"\\r\\n\", \" \", data\")). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88aef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "with open(\"chapter_2_80.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d5165",
   "metadata": {},
   "source": [
    "#### B. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.draw import draw_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbde5aa",
   "metadata": {},
   "source": [
    "#### C. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "sentences = sent_tokenize(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f17bed",
   "metadata": {},
   "source": [
    "#### D. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41881058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "token_sentences = [word_tokenize(sentence) for sentence in sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02623c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3b510",
   "metadata": {},
   "source": [
    "#### E. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ee595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d737ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "pos_sentences = [nltk.pos_tag(sentence) for sentence in token_sentences ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5103d",
   "metadata": {},
   "source": [
    "#### F. Chunking and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62c9b6",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e0cd6",
   "metadata": {},
   "source": [
    "Try extracting a sentence that you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "sentence = pos_sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ca5e8",
   "metadata": {},
   "source": [
    "Now create a tree out of that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a647d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee490b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in chunked_sentences:\n",
    "    for chunk in sent: \n",
    "        if hasattr(chunk,'label'): \n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28f327",
   "metadata": {},
   "source": [
    "#### G. Transforming that into a list and creating three different lists: 1. Person, 2. Organization, 3. GPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02935000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f577b7",
   "metadata": {},
   "source": [
    "1. Creating a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f35328",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences) #remember to always write this again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            named_entities.append((chunk.label(), ' '.join(c[0] for c in chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd08589",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602bc18",
   "metadata": {},
   "source": [
    "2. Creating a person list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = []\n",
    "\n",
    "for a,b in named_entities:\n",
    "    if a == \"PERSON\":\n",
    "        person.append([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d9646",
   "metadata": {},
   "source": [
    "3. Creating a GPE list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b90436",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE = []\n",
    "\n",
    "for a,b in named_entities:\n",
    "    if a == \"GPE\":\n",
    "        GPE.append([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10b288",
   "metadata": {},
   "source": [
    "4. Creating an organization list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "organization = []\n",
    "\n",
    "for a,b in named_entities:\n",
    "    if a == \"ORGANIZATION\":\n",
    "        organization.append([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b0e8e",
   "metadata": {},
   "source": [
    "# Exercise 2: Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72592b18",
   "metadata": {},
   "source": [
    "Now let's repeat the exercise with Spacy to compare the performance of both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759d841",
   "metadata": {},
   "source": [
    "#### A. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affaa3ef",
   "metadata": {},
   "source": [
    "#### B. We download the French SPACY pipeline and we inspect the entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98df1f",
   "metadata": {},
   "source": [
    "You may need to do this (remove the #symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b62bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df098e3",
   "metadata": {},
   "source": [
    "#### C. We initialize the NLP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2254847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d398de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "doc = nlp(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c155ec",
   "metadata": {},
   "source": [
    "#### D. We create a list with the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abe950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "named_entities = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    named_entities.append([ent.text, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a3a6a",
   "metadata": {},
   "source": [
    "#### E. We create three lists: one with person (PERSON), one with Geopolitical Entities (GPE), one with Organization (ORG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ece4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "person = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        person.append([ent.text, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "GPE = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        GPE.append([ent.text, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "org = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":\n",
    "        org.append([ent.text, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd119748",
   "metadata": {},
   "outputs": [],
   "source": [
    "org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28236a04",
   "metadata": {},
   "source": [
    "So: once again we see that Spacy really outperforms NLTK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39aecd",
   "metadata": {},
   "source": [
    "### Inhalte aus: Information Extraction.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe466a",
   "metadata": {},
   "source": [
    "# Information Extraction: NLTK and Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d178f",
   "metadata": {},
   "source": [
    "Script Sources:\n",
    "\n",
    "* **NLTK**: Tsilimos, Maria. Python: Introduction to Natural Language Processing (NLP). IT Central, University of Zurich.\n",
    "* **Spacy**: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46d1b3",
   "metadata": {},
   "source": [
    "**Information Extraction (IE)** consists on transforming **Natural Language unstructured data** (written or spoken) into **structured data** ready to be used by machines. \n",
    "\n",
    "In this notebook we are going to learn two different IE methods: **Part of Speech Tagging (POS)** and **Name Entity Recognition (NER)**.\n",
    "\n",
    "There are many excellent Python libraries out there to write scripts that will allow us to do both things. In this notebook we will learn how to use **NLTK** and **Spacy** and understand the advantages and disadvantages of both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80709b8c",
   "metadata": {},
   "source": [
    "# 1. Importing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a82776",
   "metadata": {},
   "source": [
    "Let's begin by using the first chapter of **Around the World in Eighty Days** by Jules Verne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df801e2",
   "metadata": {},
   "source": [
    "If you remember, in the previous chapter we did 4 steps of cleaning and pre-processing:\n",
    "\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Removing Punctuation\n",
    "* Removing Stopwords\n",
    "\n",
    "Now **we are not going to do any of those things**. We need to do **POS tagging**, and for that, it is necessary to keep punctuation and stopwords to avoid confusing the parser. \n",
    "\n",
    "The only thing that we are going to remove are the noisy characters \"\\r\\n\".\n",
    "\n",
    "For that, we are going to use this script: **re.sub(r\"\\r\\n\", \" \", data\")**. (in case you want to replicate it on your own dataset). \n",
    "\n",
    "For efficiency purposes a clean first chapter has been created for you with that process already incorporated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chapter_1_80.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3511207",
   "metadata": {},
   "source": [
    "# 2. Understanding Information Extraction Architecture: NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e11c1e",
   "metadata": {},
   "source": [
    "### A. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.draw import draw_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e5bf4",
   "metadata": {},
   "source": [
    "### B. We initialize the Information Extracture Pipeline:\n",
    "\n",
    "1. Sentence Segmentation\n",
    "2. Tokenization\n",
    "3. POS Tagging\n",
    "4. Chunking\n",
    "5. NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9120666",
   "metadata": {},
   "source": [
    "#### 1. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b34b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(data) \n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c094e",
   "metadata": {},
   "source": [
    "#### 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ffcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sentences = [word_tokenize(sentence) for sentence in sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75835961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b2921",
   "metadata": {},
   "source": [
    "#### 3. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = [nltk.pos_tag(sentence) for sentence in token_sentences ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f250dcb",
   "metadata": {},
   "source": [
    "#### 4. Chunking and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e6c0f",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791df901",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = pos_sentences[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68489ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc31ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c962a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b143ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a49abe",
   "metadata": {},
   "source": [
    "#### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf05dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in chunked_sentences:\n",
    "    for chunk in sent: \n",
    "        if hasattr(chunk,'label'): \n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19590fa",
   "metadata": {},
   "source": [
    "And now let's transform that into a list!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319302b9",
   "metadata": {},
   "source": [
    "Source = https://nanonets.com/blog/named-entity-recognition-with-nltk-and-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2075ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            named_entities.append((chunk.label(), ' '.join(c[0] for c in chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = []\n",
    "\n",
    "for a,b in named_entities:\n",
    "    if a == \"PERSON\":\n",
    "        person.append([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb08cad",
   "metadata": {},
   "source": [
    "That looks good so far! Let's now check **Geopolitical Entities (GPE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE = []\n",
    "\n",
    "for a,b in named_entities:\n",
    "    if a == \"GPE\":\n",
    "        GPE.append([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bae047",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0996af",
   "metadata": {},
   "source": [
    "That also looks quite good! However we observe some **issues**: is American or Londoner a person or a GPE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "organization = []\n",
    "\n",
    "for a,b in named_entities:\n",
    "    if a == \"ORGANIZATION\":\n",
    "        organization.append([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b204fd",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc72d0",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81b6ec",
   "metadata": {},
   "source": [
    "And now let's try Spacy. Spacy does not follow the same architecture as NLTK: we don´t need to follow the 4 step pipeline (sentence segmentation, tokenization, POS tagging, NER chunking). All of that is implemented in their code! Have a look at: https://spacy.io/usage/linguistic-features#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8b8c9",
   "metadata": {},
   "source": [
    "You may need to install the Spacy pipeline. If so, remove the #symbol in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0866ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe721e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5729741",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fafe18",
   "metadata": {},
   "source": [
    "Let's first have a look at the existing Entity Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3567d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"ORG\":\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a3735",
   "metadata": {},
   "source": [
    "We have a winner!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533587e",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada21e9c",
   "metadata": {},
   "source": [
    "### Inhalte aus: Mapping Jules Verne. NER with Spacy.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6576df7",
   "metadata": {},
   "source": [
    "Now that we have done things at the chapter level, let's do it at the book level! Let's focus on mapping geographically the world of Jules verne by extracting GPE and LOC of **Around the World in 80 days**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349060c",
   "metadata": {},
   "source": [
    "# 1. We import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a61087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb77cc7",
   "metadata": {},
   "source": [
    "# 2. We get our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df2800",
   "metadata": {},
   "source": [
    "This data has not been cleaned and pre-processed to avoid confusing the parser (only \\r\\n characters have been removed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"around_the_world.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c302aa",
   "metadata": {},
   "source": [
    "# 3. We import the English pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8edffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5677c",
   "metadata": {},
   "source": [
    "# 4. We create the Spacy nlp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1599a",
   "metadata": {},
   "source": [
    "# 5. We inspect the English model labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa858d",
   "metadata": {},
   "source": [
    "Let's remember the entities that we have in Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.get_pipe('ner').labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c6ef3",
   "metadata": {},
   "source": [
    "# 6. We print the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc8b0a",
   "metadata": {},
   "source": [
    "# 7. We create one list with GPE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6929d6",
   "metadata": {},
   "source": [
    "While possibly LOC is a lable that contains interesting information, as this is a DH introductory course, let's just focus on GPE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11063d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        GPE.append([ent.text, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd3171",
   "metadata": {},
   "source": [
    "Now let's drop the duplicates in there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPE_places = []\n",
    "\n",
    "for a, b in GPE:\n",
    "    GPE_places.append(a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_GPE = set(GPE_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061453c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26d141",
   "metadata": {},
   "source": [
    "Let's save our values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a86dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"GPE_aroundtheworld.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "    f.write(str(unique_GPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905024c",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7712a5",
   "metadata": {},
   "source": [
    "### Inhalte aus: Geospatial Analysis.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8681f",
   "metadata": {},
   "source": [
    "# Using Geospatial Analysis to visually analyze Travel Literature!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2032b",
   "metadata": {},
   "source": [
    "Geospatial Analysis can be a great tool to help us digg into the textual analysis of Literary Text. This can be particularly useful if we want to add extra layers of analysis to some genres such as **Travel Literature**. In this notebook we are going to exolore how to use the Python Library Plotly: https://plotly.com/python/getting-started/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94e731",
   "metadata": {},
   "source": [
    "**Sources:** the majority of the scripts in this notebook come from these sources from plotly: https://plotly.com/python/mapbox-layers/, https://plotly.com/python/scatter-plots-on-maps/, https://plotly.com/python/mapbox-layers/, https://plotly.com/python/reference/scattermapbox/#scattermapbox-marker-symbol. For more senior scripts about geo-spatial data science, this is an excellent course: https://github.com/suneman/socialdata2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c57a9",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c19247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab947d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as py \n",
    "from plotly.figure_factory import create_table # for creating nice table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9d7c6",
   "metadata": {},
   "source": [
    "# 2. We manually inspect our city dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b9682",
   "metadata": {},
   "source": [
    "All Digital Humanities projects involve some degree of close reading analysis. We need to inspect our \"GPE_aroundtheworld.txt\" file and decide which cities we are going to include in our selection! (you will see that there is a considereable ammount of noise even using Spacy, or that some place names are contemporary to the age of Jules Verne but have changed ever since)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd5d94",
   "metadata": {},
   "source": [
    "# 3. We create our GPS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b8224",
   "metadata": {},
   "source": [
    "To be able to map our cities, we need to extensively google the Latitude and Longitude of all of them, and manually annotate the results in several lists (as we will need to create a CSV dataframe to be able to plot things in maps with Plotly).\n",
    "\n",
    "Be aware that:\n",
    "\n",
    "**GPS Lat-Long signs: N+, S-, W-, E+.**\n",
    "\n",
    "For example:\n",
    "\n",
    "Rio de Janeiro: 22.9068° S, 43.1729° W (-22.9068, -43.1729)\n",
    "London: 51.5072° N, 0.1276° W (51.5072, -0.1276)\n",
    "Stockholm: 59.3293° N, 18.0686° E (59.3293, 18.0686)\n",
    "Sydney: 25.2744° S, 133.7751° E (-25.2744, -133.7751)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e35fe1",
   "metadata": {},
   "source": [
    "# Activity for you\n",
    "\n",
    "Please google \"Lat Long decimal\" and add the coordinates of **Denver, Bloomington (Indiana), Sacramento**. Add the lattitude, the longitude, and the country (at each corresponding list). Remember to remove the dots (that is just to indicate you where you should be writing things) and to write the closing braket of the list! Once you are finished run the scripts and you will automatically have a Pandas dataframe with all the information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = [\"Huangpu River (Shanghai River)\", \"Salt Lake City\", \"Lima (Peru)\", \"Green Creek\", \"Hong Kong\", \"Rewa (Vindhias)\",\n",
    "          \"San Francisco\", \"Aurangabad\", \"Macao\", \"New York\", \"Omaha\", \"Philadelphia (Pensilvania)\", \"Dublin\", \"Turin\", \n",
    "          \"Burhampoor\", \"Stockholm (Sweden)\", \"Golconda\", \"Dover\", \"Bundelkhand (Bundelcund)\", \"Ganges\", \"Bihar (Behar)\", \n",
    "          \"Havre\", \"Tokyo (Japanese Empire)\", \"Surat\", \"Fire Island\", \"Birmingham\", \"Cairo (Egypt)\", \"Paris (France)\", \n",
    "          \"Formosa\", \"Yokohama\", \"Brasilia (Brasil)\", \"Cambridge (Kirkland)\", \"Weber River\", \"Desmoines (Iowa)\", \n",
    "          \"Jackson (Missisipi)\", \"Pittsburg\", \"Mexico City (Mexico)\", \"Brindisi\", \"Jersey City\", \"Canberra (New Holland)\",\n",
    "          \"London\", \"Portland (Oregon)\", \"Murshedabad\", \"Singapur\", \"Victoria\", \"New Hampshire (Vermont)\", \"Glasgow\",\n",
    "          \"Calcutta\", \"Malacca\", \"Kansas City (Kansas)\", \"Edinburgh\", \"Carson City (Nevada)\", \n",
    "          \"Lawrence Kansas (Fort Saunders)\", \"Rock Island (Illinois)\", \"the Strait of Bab-el-Mandeb (Bab-el-Mandeb)\",\n",
    "          \"Hamburg\", \"Oslo\", \"Little Rock (Arkansas)\", \"Khandallah\", \"Nagasaki\", \"Mumbai\", \"Queenstown\", \"Edo (Yeddo)\", \n",
    "          \"Odgen\", \"Amman (Jordan)\", \"Burdwan\", \"Yokohama\", \"Oakland\", \"Marylebone\", \"Greenwich\", \"Columbus\",\n",
    "          \"Reno\", \"Amsterdam (Holland)\", \"Chicago (Illinois)\", \"Aden\", \"Cheyenne (Wyoming)\", \"Bardhaman (Burdivan)\", \n",
    "          \"Paris\", \"Liverpool\", \"Elephanta Island\", \"Southampton\", \"Long Island\", \"Fort Wayne\", \"Saddle Peak\", \n",
    "          \"Allahaban\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462cc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = [31.267401, 40.758701, -12.046374, 41.344525, 22.302711, 24.530727,  37.773972, 19.901054, 22.210928,  40.730610,\n",
    "        41.257160, 39.952583,  53.350140, 45.116177, 21.307373, 59.334591, 17.383336111111, 39.161079,  25.4556,\n",
    "       29.7666636, 25.612677, 49.490002, 35.652832, 21.170240, 40.630239, 52.489471, 30.033333, 48.864716, -26.18948040,\n",
    "       35.443707, -15.793889, 42.373611, 40.71578,  41.619549, 35.514706, 40.440624, 19.432608, 40.633331, 40.719074,\n",
    "       -35.282001, 51.509865,  45.523064, 24.175903,  1.290270,  -37.020100, 44.000000, 55.860916, 22.572645, \n",
    "       2.200844, 39.106667,  55.953251, 39.1638, 38.960213, 41.487076, 12.583, 53.551086, 59.911491, 34.746483, \n",
    "       -41.24500000, 32.764233, 19.076090, -45.031162, 35.822994, 34.273178, 31.963158, 23.232513, 35.443707,\n",
    "       37.804363, 51.518875, 51.477928, 39.983334,  39.530895, 52.377956, 41.881832, 12.800000, 41.161079, \n",
    "       23.232513,  48.864716,  53.400002,  18.963253,  50.909698, 40.792240, 41.093842, 11.623377, 25.473034]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = [121.522179, 111.876183, -77.042793, -82.968471, 114.177216,  81.299110, -122.431297, 75.352478, 113.552971, \n",
    "       -73.935242, -95.995102, -75.165222,  -6.266155, 7.742615, 76.230415, 18.063240, 78.404169444444, -75.525681, \n",
    "       78.5636, 78.1999992, 85.158875, 0.100000, 139.839478, 72.831062, -73.308549,  -1.898575,  31.233334, 2.349014,\n",
    "       -58.22428060, 139.638031, -47.882778, -71.110558,  -110.898227, -93.598022,  -89.912506, -79.995888,  -99.133209,\n",
    "        17.933332, -74.050552, 149.128998, -0.118092, -122.676483, 88.280182, 103.851959, 144.964600, -72.699997, \n",
    "       -4.251433, 88.363892, 102.240143, -94.676392, -3.188267,  -119.7674, -95.277390,  -90.589691, 43.417, \n",
    "       9.993682, 10.757933,  -92.289597, 174.79422000, 129.872696, 72.877426, 168.662643,  139.753493,  -77.818047, \n",
    "        35.930359,  87.863419, 139.638031, -122.271111, -0.149895, -0.001545, -82.983330, -119.814972,  4.897070, \n",
    "        -87.623177, 45.033333, -104.805450, 87.863419, 2.349014,  -2.983333,  72.931442, -1.404351, -73.138260, \n",
    "       -85.139236,  92.726486, 81.878357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"China\", \"USA\", \"Peru\", \"USA\", \"Hong Kong\", \"India\", \"USA\", \"India\", \"China\", \"USA\", \"USA\", \"USA\", \"Ireland\", \n",
    "             \"Italy\", \"India\", \"Sweden\", \"India\", \"UK\", \"India\", \"India\", \"India\", \"France\", \"Japan\", \"India\", \"USA\", \n",
    "              \"UK\", \"Egypt\", \"France\", \"Argentina\", \"Japan\", \"Brazil\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"Mexico\",\n",
    "             \"Italy\", \"USA\", \"Australia\", \"UK\", \"USA\", \"India\", \"Singapore\", \"Australia\", \"USA\", \"UK\", \"India\", \"Malaysia\",\n",
    "             \"USA\", \"UK\", \"USA\", \"USA\", \"USA\", \"Ocean\", \"Germany\", \"Norway\", \"USA\", \"New Zealand\", \"Japan\", \"India\", \n",
    "             \"Australia\", \"Japan\", \"USA\", \"Jordan\", \"India\", \"Japan\", \"USA\", \"UK\", \"UK\", \"USA\", \"USA\", \"Netherlands\", \n",
    "             \"USA\", \"Yemen\", \"USA\", \"India\", \"France\", \"UK\", \"India\", \"UK\", \"USA\", \"USA\", \"India\", \"India\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0858db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(places, columns = [\"cities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lat\"] = lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lon\"] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c95f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"countries\"] = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b27dd4",
   "metadata": {},
   "source": [
    "# Geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189cc34",
   "metadata": {},
   "source": [
    "And now let's try another python library called GEOPY that will tell us the coordinates of our cities! If you are curious, you can read the documentation in here: https://geopy.readthedocs.io/en/stable/. For a faster tutorial you can have a look at https://pypi.org/project/geopy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e38dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Nominatim geocoder\n",
    "geolocator = Nominatim(user_agent=\"MyApp\", timeout = 5)\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098fdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose a city\n",
    "location = geolocator.geocode(\"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aae928",
   "metadata": {},
   "outputs": [],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The latitude of the location is: \", location.latitude)\n",
    "print(\"The longitude of the location is: \", location.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris = [location.latitude, location.longitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f522c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db99eca",
   "metadata": {},
   "source": [
    "Let's scale that to our full dataset of cities (so: if we have a file with all the GPE locations, we feed it into this script and it wil be super fast!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114782e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in places:\n",
    "    location = geolocator.geocode(city)\n",
    "    if location:\n",
    "        city_coords.append((location.point.latitude, location.point.longitude))\n",
    "    else:\n",
    "        city_coords.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320abff4",
   "metadata": {},
   "source": [
    "When we get a none message it means that geopy does not know where is that city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113dd8b5",
   "metadata": {},
   "source": [
    "# 5. And now we visualize things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3c32c",
   "metadata": {},
   "source": [
    "Let's first try this map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832feb24",
   "metadata": {},
   "source": [
    "#### A. Mapbox Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac276746",
   "metadata": {},
   "source": [
    "Mapbox maps are also called tile-based maps and they allow you to zoom in \"google maps\" style. For more information have a look at: https://plotly.com/python/mapbox-layers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7ce87",
   "metadata": {},
   "source": [
    "### Activity for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3598a0",
   "metadata": {},
   "source": [
    "Change the color_discrete_sequence = [] variable from \"fuschia\" to \"green\". You can try other colours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(data, lat=\"lat\", lon=\"lon\", hover_name=\"cities\", hover_data=[\"countries\"], #this is the text\n",
    "                        color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)                     #that goes inside the boxes\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1b5c2",
   "metadata": {},
   "source": [
    "#### Activity for you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90779211",
   "metadata": {},
   "source": [
    "Move around your mouse on the top right corner of the map and click on the picture camera, where it says \"Download plot as PNG\". You will be able to download your map in your own laptop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de555ef",
   "metadata": {},
   "source": [
    "#### B. Geo maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8b290",
   "metadata": {},
   "source": [
    "Geo Maps only show the physical boundaries of countries. Have a look at: https://plotly.com/python/map-configuration/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c67817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['cities'] + ', ' + data[\"countries\"].astype(str)\n",
    "\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "        lon = data['lon'],\n",
    "        lat = data['lat'],\n",
    "        text = data['text'],\n",
    "        mode = 'markers',\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "        title = 'Around the World in 80 days',\n",
    "        geo_scope='world',\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0678c",
   "metadata": {},
   "source": [
    "Which one do you like the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de74fd6",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed55e06",
   "metadata": {},
   "source": [
    "### Inhalte aus: Harry Potter around the World.ipynb\n",
    "#### Zusammenfassung und Erklärung auf Deutsch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95383ee8",
   "metadata": {},
   "source": [
    "# Mapping the world of Harry Potter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7c2ce",
   "metadata": {},
   "source": [
    "**Script source:** several queries to Perplexity AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d22c68",
   "metadata": {},
   "source": [
    "Harry Potter is one of the most translated (and popular) books around the world and it is available in 85 languages! (https://en.wikipedia.org/wiki/List_of_Harry_Potter_translations)\n",
    "\n",
    "Let's write a python script to do a geo-spatial analysis visualization of things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f2af4",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as py \n",
    "from plotly.figure_factory import create_table\n",
    "\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235e003",
   "metadata": {},
   "source": [
    "# 2. We create a variable with the capitals of the countries where Harry Potter has been translated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fd275",
   "metadata": {},
   "source": [
    "In the real world you would need to do this step yourself! How would you do this using Python?\n",
    "\n",
    "And: every time there is more than one language in a country (i.e. South Africa: English and Afrikaans) I have used two cities in that country (i.e Pretoria and Cape Town) to show linguistic diversity!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"London\", \"Dublin\", \"Sydney\", \"Wellington\", \"Toronto\", \"Pretoria\", \"Cape Town\", \"Washington DC\", \"New Dehli\", \"Kuala Lumpur\", \n",
    "          \"Manila\", \"Singapore\", \"Tirana\", \"Pristina\", \"Cairo\", \"Yerevan\", \"Oviedo\", \"Baku\", \"Bilbao\", \"Minsk\", \"Dhaka\", \"Sarajevo\", \n",
    "          \"Rennes\", \"Sofia\", \"Barcelona\", \"Beijing\", \"Taiwan\", \"Hong Kong\", \"Macau\", \"Taiwan\", \"Zagreb\", \"Prague\", \"Copenhagen\", \n",
    "          \"Ghent\", \"Amsterdam\", \"Paramaribo\", \"Tallinn\", \"Tórshavn\", \"Pasay\", \"Helsinki\", \"Brussels\", \"Quebec\", \"Paris\", \"Monaco\",\n",
    "          \"Lausanne\", \"Luxembourg\", \"Leeuwarden\", \"Santiago\", \"Tbilisi\", \"Vienna\", \"Berlin\", \"Vaduz\", \"Zurich\", \"Echternach\",\n",
    "          \"Hamburg\", \"Athens\", \"Thessaloniki\", \"Nuuk\", \"Gandhinagar\", \"Honolulu\", \"Jerusalem\", \"Mumbai\", \"Budapest\", \n",
    "          \"Reykjavik\", \"Jakarta\", \"Galway\", \"Belfast\", \"Rome\", \"San Marino\", \"Lugano\", \"Tokyo\", \"Phnom Penh\", \"Lahore\", \n",
    "          \"Seoul\", \"Milan\", \"Riga\", \"Vilnius\", \"Diekirch\", \"Skopje\", \"Kuantan\", \"Thiruvananthapuram\", \"Auckland\", \n",
    "          \"Nagpur\", \"Ulaanbaatar\", \"Kathmandu\", \"Trondheim\", \"Bourdeaux\", \"Girona\", \"Tehran\", \"Warsaw\", \"Lisboa\", \n",
    "          \"Brasília\", \"Bucharest\", \"Chișinău\", \"Moscow\", \"Edinburgh\", \"Belgrade\", \"Podgorica\", \"Trebinje\", \"Colombo\", \n",
    "          \"Bratislava\", \"Ljubljana\", \"Madrid\", \"Rosario\", \"Buenos Aires\", \"Stockholm\", \"Nyland\", \"Chennai\", \"Amaravati\",\n",
    "          \"Bangkok\", \"Lhasa\", \"Ankara\", \"Kyiv\", \"Islamabad\", \"Hanoi\", \"Cardiff\", \"Jerusalem\", \"Mostar\", \"New York City\", \n",
    "          \"Utrecht\", \"Krakow\", \"Sibiu\", \"Gothenborg\", \"Horlivka\"]\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4df113",
   "metadata": {},
   "source": [
    "# 3. We create a list with the Lattitude and Longitude of those cities using Geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295d458",
   "metadata": {},
   "source": [
    "Let's first practice getting the lat and lon of 3 English speaking main cities: London, Dublin, and New York City. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"MyApp\", timeout = 5)\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "london = geolocator.geocode(\"London\")\n",
    "dublin = geolocator.geocode(\"Dublin\")\n",
    "nyc = geolocator.geocode(\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d855ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "london"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ced9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The lat and long of {london} is: \", london.latitude, london.longitude)\n",
    "print(f\"The lat and long of {dublin} is: \", dublin.latitude, dublin.longitude)\n",
    "print(f\"The lat and long of {nyc} is: \", nyc.latitude, nyc.longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66073b2a",
   "metadata": {},
   "source": [
    "Now let's do that for all the cities in our list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd9803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "city_coords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    location = geolocator.geocode(city)\n",
    "    if location:\n",
    "        city_coords.append((location.point.latitude, location.point.longitude))\n",
    "    else:\n",
    "        city_coords.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd7898",
   "metadata": {},
   "source": [
    "# 4. Pandas Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f88d6",
   "metadata": {},
   "source": [
    "Now let's create a Pandas Dataframe that contains our cities and their lat and lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f63ae9",
   "metadata": {},
   "source": [
    "First let's create a column with the names of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14dbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12838d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "harry_potter = pd.DataFrame(cities, columns = [\"Cities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afe6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_potter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a781fa",
   "metadata": {},
   "source": [
    "Now create two variables: one for latituted and one for longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac70eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18efa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(city_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd09de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(city_coords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6442d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = [x[0] for x in city_coords]\n",
    "lon = [x[1] for x in city_coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07469187",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c20291",
   "metadata": {},
   "source": [
    "And now let's add those columns to our data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e83d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "harry_potter[\"lat\"] = lat\n",
    "harry_potter[\"lon\"] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf38573",
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_potter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d9917",
   "metadata": {},
   "source": [
    "# 5. And now let's visualize things!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fedae",
   "metadata": {},
   "source": [
    "Change the colour for red (for Gryffindor!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5623fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(harry_potter, lat=\"lat\", lon=\"lon\", hover_name=\"Cities\", #this is the text\n",
    "                        color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)                     #that goes inside the boxes\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8dd54c",
   "metadata": {},
   "source": [
    "And now change the colour for green (for Slytherin!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assume 'harry_potter' is your data frame with 'lon', 'lat', and 'text' columns\n",
    "harry_potter['marker_color'] = 'blue'  # Assign a default color\n",
    "harry_potter.loc[harry_potter['Cities'].str.contains('Harry Potter'), 'marker_color'] = 'red'  # Change color for specific text\n",
    "\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "    lon = harry_potter['lon'],\n",
    "    lat = harry_potter['lat'],\n",
    "    text = harry_potter['Cities'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        color = harry_potter['marker_color'])))\n",
    "\n",
    "fig.update_layout(\n",
    "    title = 'Harry Potter Translations',\n",
    "    geo_scope = 'world'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
